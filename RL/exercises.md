- [绪论](#绪论)
- [多臂赌博机问题](#多臂赌博机问题)
- [马尔可夫决策过程](#马尔可夫决策过程)
- [表格型方法](#表格型方法)
- [动态规划](#动态规划)
- [蒙特卡洛方法](#蒙特卡洛方法)
- [时序差分学习](#时序差分学习)
- [$n$步引导（Bootstrapping）方法](#$n$步引导（Bootstrapping）方法)
- [表格方法规划和学习](#表格方法规划和学习)
- [策略梯度](#策略梯度)
- [资格迹](#资格迹)
# 绪论
## 习题
1. **强化学习的基本结构是什么？**
	
	本质上是智能体与环境的交互。具体地，当智能体在环境中得到当前时刻的状态后，其会基于此状态输出一个动作，这个动作会在环境中被执行并输出下一个状态和当前的这个动作得到的奖励。智能体在环境里存在的目标是最大化期望累积奖励。
2. **强化学习相对于监督学习为什么训练过程会更加困难？**
	1. 强化学习处理的大多是序列数据，其很难像监督学习的样本一样满足独立同分布条件。
	2. 强化学习有奖励的延迟，即智能体的动作作用在环境中时，环境对于智能体状态的奖励存在延迟，使得反馈不实时。
	3. 监督学习有正确的标签，模型可以通过标签修正自己的预测来更新模型，而强化学习相当于一个“试错”的过程，其完全根据环境的“反馈”更新对自己最有利的动作。
3. **强化学习的基本特征有哪些？**
	1. 有试错探索过程，即需要通过探索环境来获取对当前环境的理解。
	2. 强化学习中的智能体会从环境中获得延迟奖励。
	3. 强化学习的训练过程中时间非常重要，因为数据都是时间关联的，而不是像监督学习中的数据大部分是满足独立同分布的。
	4. 强化学习中智能体的动作会影响它从环境中得到的反馈。
4. **近几年强化学习发展迅速的原因有哪些？**
	1. 算力的提升使我们可以更快地通过试错等方法来使得智能体在环境里面获得更多的信息，从而取得更大的奖励。
	2. 我们有了深度强化学习这样一个端到端的训练方法，可以把特征提取、价值估计以及决策部分一起优化，这样就可以得到一个更强的决策网络。
5. **状态和观测有什么关系？**
	
	状态是对环境的完整描述，不会隐藏环境信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用同一个实值向量、矩阵或者更高阶的张量来表示状态和观测。
6. **一个强化学习智能体由什么组成？**
	1. 策略函数，智能体会用策略函数来选取它下一步的动作，策略包括随机性策略和确定性策略。
	2. 价值函数，我们用价值函数来对当前状态进行评估，即进入现在的状态可以对后面的奖励带来多大的影响。价值函数的值越大，说明进入该状态越有利。
	3. 模型，其表示智能体对当前环境状态的理解，它决定系统是如何运行的。
7. **根据强化学习智能体的不同，我们可以将其分为哪几类？**
	1. 基于价值的智能体。显式学习的是价值函数，隐式地学习智能体的策略。因为这个策略是从学到的价值函数里面推算出来的
	2. 基于策略的智能体。其直接学习策略，即直接给智能体一个状态，它就会输出对应动作的概率。当然在基于策略的智能体里面并没有去学习智能体的价值函数。
	3. 另外还有一种智能体，它把以上两者结合。把基于价值和基于策略的智能体结合起来就有了演员-评论员智能体。这一类智能体通过学习策略函数和价值函数以及两者的交互得到更佳的状态。
8. **基于策略迭代和基于价值迭代的强化学习方法有什么区别？**
	1. 基于策略迭代的强化学习方法，智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据该策略进行操作。强化学习算法直接对策略进行优化，使得制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。
	2. 基于价值迭代的方法只能应用在离散的环境下，例如围棋或某些游戏领域，对于行为集合规模庞大或是动作连续的场景，如机器人控制领域，其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。
	3. 基于价值迭代的强化学习算法有Q-learning、Sarsa等，基于策略迭代的强化学习算法有策略梯度算法等。
	4. 此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，从而取得更好的效果。
9. **有模型学习和免模型学习有什么区别？**
	
	针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。总体来说，有模型学习相比免模型学习仅仅多出一个步骤，即对真实环境进行建模。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。
10. **如何通俗理解强化学习？**
	
	环境和奖励函数不是我们可以控制的，两者是在开始学习之前就已经事先确定的。我们唯一能做的事情是调整策略，使得智能体可以在环境中得到最大的奖励。另外，策略决定了智能体的行为，策略就是给一个外界的输入，然后它会输出现在应该要执行的动作。
## 面试题
1. **看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？**
	
	强化学习包含环境、动作和奖励3部分，其本质是智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大，或者说是期望最大。
2. **请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？**
	
	首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建和训练。其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。另外，强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独立同分布条件。
3. **根据你的理解，你认为强化学习的使用场景有哪些呢？**
	
	7个字总结就是“多序列决策问题”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达的条件，即满足可学习条件。
4. **请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？**
	
	深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大。
5. **你了解有模型和免模型吗？两者具体有什么区别呢？**
	
	我认为两者的区别主要在于是否需要对真实的环境进行建模，免模型方法不需要对环境进行建模，直接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如果建模的环境与真实环境的差异较大，那么会限制其泛化性能。现在通常使用有模型方法进行模型的构建工作。
# 多臂赌博机问题
## 练习
1. **在$\epsilon$贪婪动作选择中，对于两个动作的情况和$\epsilon = 0.5$，选择贪婪动作的概率是多少？**
	
	$1 - \epsilon + \epsilon\times\frac12 = 1 - 0.5 + 0.5\times\frac12 = \frac34$
2. **赌博机示例 考虑具有$k = 4$动作的$k$臂赌博机问题，表示为1，2，3和4。对于此问题，考虑使用$\epsilon$贪婪动作选择，样本平均动作值估计的赌博机算法，对于所有a，初始估计为$Q_1(a) = 0$。假设动作和奖励的初始序列是$A_1 = 1$，$R_1 = 1$，$A_2 = 2$，$R_2 = 1$，$A_3 = 2$，$R_3 = 2$，$A_4 = 2$，$R_4 = 2$，$A_5 = 3$，$R_5 = 0$。在某些时间步骤中，$\epsilon$情况可能已经发生，导致随机选择动作。在哪个时间步骤确实发生了？在哪些时间步骤可能发生？**
	
	在时间步骤2确实发生了。在时间步骤1、3、4、5可能发生。
3. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.2.png)
	
	**图** 10臂赌博机试验中的$\epsilon$贪婪行动价值方法的平均表现。 这些数据是2000轮以上不同赌博机问题的平均值。所有方法都使用样本平均值作为其行动价值估计。
	
	**在图所示的比较中，从累积奖励和选择最佳动作的概率来看，哪种方法在长期运行中表现最佳？会有多好？定量地表达你的答案。**
	
	$\epsilon = 0.01$
4. $$Q_{n + 1} = Q_n + \alpha(R_n - Q_n) = (1 - \alpha)^nQ_1 + \sum_{i = 1}^n\alpha(1 - \alpha)^{n - i}R_i$$
	**如果步长参数$\alpha_n$不是常数，则估计值$Q_n$是先前收到的奖励的加权平均值，其权重不同于给出的权重。对于一般情况，就步长参数的顺序而言，每个先前奖励的权重是什么？**
	
	$\alpha_i\prod_{j = i + 1}^n(1 - \alpha_j)R_i$
5. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.3.png)
	
	**图** 乐观的初始行动价值估计对10臂赌博机试验的影响。两种方法都使用恒定的步长参数，$\alpha = 0.1$。
	
	**_神秘的尖峰_ 图所示的结果应该非常可靠，因为它们是超过2000个随机选择的10臂赌博机任务的平均值。那么，为什么乐观方法曲线的早期会出现振荡和峰值？换句话说，什么可能使这种方法在特定的早期步骤中表现得更好或更差？**
6. **_无偏恒定步长技巧_ 在本章的大部分内容中，我们使用样本平均值来估计动作值，因为样本平均值不会产生恒定步长的初始偏差。然而，样本平均值并不是一个完全令人满意的解决方案，因为它们可能在非平稳问题上表现不佳。是否有可能避免不变步长的偏差，同时保留其对非平稳问题的优势？一种方法是使用步长$\beta_n = \alpha / \bar o_n$处理特定动作的第n个奖励，其中$\alpha > 0$是常规常量步长，$\bar o_n$是从0开始的跟踪：$\bar o_n = \bar o_{n - 1} + \alpha(1 - \bar o_{n - 1})\text{ for }n > 0\text{, with }\bar o_0 = 0$**
	
	**进行分析，以表明$𝑄_n$是指数的新近加权平均值，_没有初始偏差_。**
	
	$Q_{n + 1} = Q_n + \alpha(R_n - Q_n) \Rightarrow Q_{n + 1} = Q_n + \beta_n(R_n - Q_n) = Q_n + (\alpha / \bar o_n)(R_n - Q_n) = Q_n + \{\alpha / [\bar o_{n - 1} + \alpha(1 - \bar o_{n - 1})]\}(R_n - Q_n) = (1 - \alpha)^n\prod_{i = 1}^n\frac{\bar o_{i - 1}}{\bar o_{i - 1} + \alpha(1 - \bar o_{i - 1})}Q_1 + \alpha\sum_{i = 1}^n\frac{(1 - \alpha)^{n - i}}{\bar o_{i - 1} + \alpha(1 - \bar o_{i - 1})}\prod_{j = i + 1}^n\frac1{\bar o_{j - 1} + \alpha(1 - \bar o_{j - 1})}R_i$
7. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.4.png)
	
	**图** 10臂赌博机试验上UCB动作选择的平均表现。如图所示，除了在前$k$个步骤中，当它在尚未尝试的动作中随机选择时，UCB通常比$\epsilon$贪婪动作选择更好。
	
	**_USB尖峰_ 在图中，UCB算法在第11步显示出明显的性能峰值。为什么是这样？请注意，为了使您的答案完全令人满意，它必须解释为什么奖励在第11步增加以及为什么在随后的步骤中减少。提示：如果$c = 1$，则尖峰不太突出。**
8. **指出在两个动作的情况下，soft-max分布与统计学和人工神经网络中经常使用的逻辑或sigmoid函数给出的相同。**
# 马尔可夫决策过程
## 习题
1. **为什么在马尔可夫奖励过程中需要有折扣因子？**
	1. 首先，是有些马尔可夫过程是环状的，它并没有终点，所以我们想避免无穷的奖励。
	2. 另外，我们想把不确定性也表示出来，希望尽可能快地得到奖励，而不是在未来的某个时刻得到奖励。
	3. 接上一点，如果这个奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面才可以得到奖励。
	4. 还有，在有些时候，折扣因子也可以设为0。当它被设为0后，我们就只关注它当前的奖励。我们也可以把它设为1，设为1表示未来获得的奖励与当前获得的奖励是一样的。
	
	所以，折扣因子可以作为强化学习智能体的一个超参数进行调整，然后就会得到不同行为的智能体。
2. **为什么矩阵形式的贝尔曼方程的解析解比较难求得？**
	
	通过矩阵求逆的过程，我们就可以把$V$的解析解求出来。但是这个矩阵求逆的过程的复杂度是$O(N^3)$，所以当状态非常多的时候，比如从10个状态到1000个状态，到100万个状态，那么当我们有100万个状态的时候，转移矩阵就会是一个100万乘100万的矩阵。对于这样一个大矩阵进行求逆是非常困难的，所以这种通过解析解去解的方法，只能应用在很小量的马尔可夫奖励过程中。
3. **计算贝尔曼方程的常见方法有哪些，它们有什么区别？**
	1. 蒙特卡洛方法：可用来计算价值函数的值。以小船示例为例，当得到一个马尔可夫奖励过程后，我们可以从某一个状态开始，把小船放到水中，让它“随波逐流”，这样就会产生一条轨迹，从而得到一个折扣后的奖励$g$ 。当积累该奖励到一定数量后，直接除以轨迹数量，就会得到其价值函数的值。
	2. 动态规划方法：可用来计算价值函数的值。通过一直迭代对应的贝尔曼方程，最后使其收敛。当最后更新的状态与上一个状态区别不大的时候，通常是小于一个阈值$\gamma$时，更新就可以停止。
	3. 以上两者的结合方法：我们也可以使用时序差分学习方法，其为动态规划方法和蒙特卡洛方法的结合。
4. **马尔可夫奖励过程与马尔可夫决策过程的区别是什么？**
	
	相对于马尔可夫奖励过程，马尔可夫决策过程多了一个决策过程，其他的定义与马尔可夫奖励过程是类似的。由于多了一个决策，多了一个动作，因此状态转移也多了一个条件，即执行一个动作，导致未来状态的变化，其不仅依赖于当前的状态，也依赖于在当前状态下智能体采取的动作决定的状态变化。对于价值函数，它也多了一个条件，多了一个当前的动作，即当前状态以及采取的动作会决定当前可能得到的奖励的多少。
	
	另外，两者之间是有转换关系的。具体来说，已知一个马尔可夫决策过程以及一个策略$\pi$时，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程中，状态的转移函数$P(s' | s, a)$是基于它的当前状态和当前动作的，因为我们现在已知策略函数，即在每一个状态，我们知道其采取每一个动作的概率，所以我们就可以直接把这个动作进行加和，就可以得到对于马尔可夫奖励过程的一个转移概率。同样地，对于奖励，我们可以把动作去掉，这样就会得到一个类似于马尔可夫奖励过程的奖励。
5. **马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的结构或者计算方面的差异有哪些？**
	
	对于马尔可夫链，它的转移概率是直接决定的，即从当前时刻的状态通过转移概率得到下一时刻的状态值。但是对于马尔可夫决策过程，其中间多了一层动作的输出，即在当前这个状态，首先要决定采取某一种动作，再通过状态转移函数变化到另外一个状态。所以在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫过程的不同之处。在马尔可夫决策过程中，动作是由智能体决定的，所以多了一个组成部分，智能体会采取动作来决定未来的状态转移。
6. **我们如何寻找最佳策略，寻找最佳策略方法有哪些？**
	
	本质来说，当我们取得最佳价值函数后，我们可以通过对$Q$函数进行最大化，从而得到最佳价值。然后，我们直接对$Q$函数取一个让动作最大化的值，就可以直接得到其最佳策略。具体方法如下，
	1. 穷举法（一般不使用）：假设我们有有限个状态、有限个动作可能性，那么每个状态我们可以采取$A$种动作策略，那么总共就是$|A|^{|S|}$个可能的策略。我们可以把他们穷举一遍，然后算出每种策略的价值函数，对比一下就可以得到最佳策略。但是这种方法的效率极低。
	2. 策略迭代：一种迭代方法，其由两部分组成，以下两个步骤一直在迭代进行，最终收敛，其过程有些类似于机器学习中的EM算法（期望-最大化算法）。第一个步骤是策略评估，即当前我们在优化这个策略$\pi$，在优化过程中通过评估从而得到一个更新的策略；第二个步骤是策略提升，即取得价值函数后，进一步推算出它的$Q$函数，得到它的最大值。
	3. 价值迭代：我们一直迭代贝尔曼最优方程，通过迭代，其能逐渐趋向于最佳策略，这是价值迭代方法的核心。我们为了得到最佳的$V^*$，对于每个状态的$V^*$值，直接使用贝尔曼最优方程进行迭代，迭代多次之后它就会收敛到最佳策略及其对应的状态，这里是没有策略函数的。
## 面试题
1. **请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？**
	
	马尔可夫过程是一个二元组$<S, P>$，$S$为状态集合，$P$为状态转移函数；马尔可夫决策过程是一个五元组$<S, P, A, R, \gamma >$，其中$R$表示从$S$到$S'$能够获得的奖励期望，$\gamma$为折扣因子，$A$为动作集合；马尔可夫最重要的性质是下一个状态只与当前状态有关，与之前的状态无关，也就是 $p(s_{t + 1} | s_t) = p(s_{t + 1} | s_1, s_2, \dots, s_t)$。
2. **请问我们一般怎么求解马尔可夫决策过程？**
	
	我们求解马尔可夫决策过程时，可以直接求解贝尔曼方程或动态规划方程：$V(s) = R(S) + \gamma\sum_{s' \in S}p(s' | s)V(s')$
	
	特别地，其矩阵形式为$V = R + \gamma PV$。但是贝尔曼方程很难求解且计算复杂度较高，所以可以使用动态规划、蒙特卡洛以及时序差分等方法求解。
3. **请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？**
	
	如果不具备马尔可夫性，即下一个状态与之前的状态也有关，若仅用当前的状态来求解决策过程，势必导致决策的泛化能力变差。为了解决这个问题，可以利用循环神经网络对历史信息建模，获得包含历史信息的状态表征，表征过程也可以使用注意力机制等手段，最后在表征状态空间求解马尔可夫决策过程问题。
4. **请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。**
	1. 基于状态价值函数的贝尔曼方程：$V_\pi(s) = \sum_a\pi(a|s)\sum_{s', r}p(s', r | s, a)(r(s, a) + \gamma V_\pi(s'))$
	2. 基于动作价值函数的贝尔曼方程：$Q_\pi(s, a) = \sum_{s', r}p(s', r|s, a)(r(s', a) + \gamma V_\pi(s'))$
5. **请问最佳价值函数$V_*$和最佳策略$π^*$为什么等价呢？**
	
	最佳价值函数的定义为$V^*(s) = \max_\pi V_\pi(s)$，即我们搜索一种策略$\pi$来让每个状态的价值最大。$V^*$就是到达每一个状态其的最大价值，同时我们得到的策略就可以说是最佳策略，即$π^*(s) = \argmax_\pi V_\pi(s)$。最佳策略使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个最佳价值函数，就可以说某一个马尔可夫决策过程的环境被解。在这种情况下，其最佳价值函数是一致的，即其达到的上限的值是一致的，但这里可能有多个最佳策略对应于相同的最佳价值。
6. **能不能手写一下第$n$步的价值函数更新公式呀？另外，当$n$越来越大时，价值函数的期望和方差是分别变大还是变小呢？**
	
	$n$越大，方差越大，期望偏差越小。价值函数的更新公式如下：$Q(S, A) \leftarrow Q(S, A) + \alpha(\sum_{i = 1}^n\gamma^{i - 1}r_{t + i} + \gamma^n\max_aQ(S', a) - Q(S, A))$
## 练习
1. **MDP框架是否足以有效地代 _所有_ 目标导向的学习任务？你能想到任何明显的例外吗？**
2. **考虑驾驶问题。你可以根据加速器，方向盘和制动器（即你的身体与机器接触的位置）来定义动作。或者你可以将它们定义得更远，比如橡胶与道路相遇，考虑你的动作是轮胎扭矩。或者你可以进一步定义它们，比如说，你的大脑掌控身体，肌肉抽搐的动作来控制你的四肢。或者你可以达到一个更高的层次，说你的行动是你选择开车的地方。什么是个体和环境之间合适的层次和位置分界？在什么基础上，该线的一个位置优先于另一个？是否有任何根本原因选择一个位置而不是另一个位置，还是随意选择？**
- **例：环保机器人**
	
	移动机器人的工作是在办公室环境中收集空的汽水罐。它有用于检测汽水罐的传感器，以及可以将它们拾起并放置在机箱中的臂和夹具；它使用可充电电池供电。机器人的控制系统具有用于解释传感器信息，用于导航以及用于控制手臂和夹具的部件。关于如何搜索汽水罐的高级决策是由强化学习个体根据电池的当前充电水平做出的。举一个简单的例子，我们假设只能区分两个电荷电平，包括一个小的状态集$\mathcal S = \{高, 低\}$。在每个状态，个体可以决定是否（1）在一段时间内主动 **搜索** 汽水罐，（2）保持静止并 **等待** 某人给它汽水罐，或（3）返回其本垒为电池 **充电**。当能量水平很**高**时，充电总是愚蠢的，所以我们不会将其包含在为此状态设定的动作中。 动作集是$\mathcal A(高) = \{搜索, 等待\}$和$\mathcal A(低) = \{搜索, 等待, 充电\}$。
	
	奖励在大多数情况下为零，但是当机器人固定空罐时变为正值，或者如果电池完全耗尽则变为负值。找到汽水罐的最好方法是主动搜索它们，但这会耗尽机器人的电池电量，而等待则不会。每当机器人正在搜索时，存在其电池耗尽的可能性。在这种情况下，机器人必须关闭并等待获救（产生低回报）。如果电池电量水平**高**，则可以始终完成一段主动搜索而没有耗尽电池的风险。以**高**电量水平开始的搜索周期使电量水平以概率$\alpha$保持并且以概率$1 - \alpha$降低至**低**电量水平。另一方面，当电量水平**低**时进行的搜索周期使其以概率$\beta$变**低**并且以概率$1 - \beta$消耗电池。在后一种情况下，必须拯救机器人，然后将电池重新充电至**高**电量水平。机器人收集的每个汽水罐都可以作为单位奖励计算，而每当机器人必须获救时，奖励为$-3$。用$r_{搜索}$和$r_{等待}$，其中$r_{搜索} > r_{等待}$，分别表示机器人在搜索和等待时将收集的预期罐数（以及预期的奖励）。最后，假设在跑步回家期间不能收集罐头，并且在电池耗尽的过程中不能收集罐头。这个系统是一个有限的MDP，我们可以记下转移概率和预期的奖励，动态如左表所示：
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/table_figure.png)
	
	请注意，表中有一行代表当前状态$s$，动作$a$，$a \in \mathcal A(s)$的每种可能组合。某些转换的概率为零，因此没有为它们指定预期的奖励。右侧所示是另一种有用的方法，可以总结有限MDP的动态，称为 _转换图_。 有两种节点：_状态节点_ 和 _动作节点_。每个可能的状态都有一个状态节点（由状态名称标记的大圆圈），以及每个状态-动作对的动作节点（由行动名称标记并由线连接的小实心圆圈）。从状态$s$开始并采取动$a$，你将沿着从状态节点$s$到动作节点$(s, a)$的线路移动。然后，环境通过离开动作节点$(s, a)$的箭头之一转换到下一个状态的节点。每个箭头对应一个三元组$(s, s', a)$，其中$s'$是下一个状态，我们用转移概率$p(s' | s, a)$标记箭头，以及该转换的预期回报$r(s, a, s')$。请注意，标记离开动作节点的箭头的转移概率和总是为$1$。
3. **给出一个类似于例中的表，但是对于$p(s', r | s, a)$。它应该有$s$，$a$，$s'$，$r$和$p(s', r | s, a)$的列，以及$p(s', r | s, a) > 0$的每个4元组的行。**
4. **$\sum_{s' \in \mathcal S}\sum_{r \in \mathcal R}p(s', r | s, a) = 1\text{，对所有}s \in \mathcal S\text{，}a \in \mathcal A(s)$等式是针对连续的情况，需要进行修改（非常轻微）以应用于情节任务。通过给出修改版本，表明你知道所需的修改。**
	
	$\sum_{s' \in \mathcal S \cup S^+}\sum_{r \in \mathcal R}p(s', r | s, a) = 1\text{，对所有}s \in \mathcal S\text{，}a \in \mathcal A(s)$
5. **假设你将杆平衡作为一个情节性任务，但是也使用了衰减因子，除了$-1$是失败之外，所有奖励都是零。那么每次回报是多少？这个回报与有衰减的持续任务有什么不同？**
	
	$-\gamma^{K - 1}$
6. **想象一下，你正在设计一个运行迷宫的机器人。你决定在逃离迷宫时奖励$+1$，在其他时候奖励零。任务似乎自然地分解为情景，即连续贯穿迷宫的运行，所以你决定把它当作一个偶然的任务，其目标是最大化预期的总奖励。运行学习个体一段时间后，您会发现它从迷宫中逃脱没有任何改善。出了什么问题？你是否有意识地向个体传达了你希望它实现的目标？**
7. **假设$\gamma = 0.5$并且接收以下奖励序列$R_1 = 1$，$R_2 = 2$， $R_3 = 6$，$R_4 = 3$，并且$R_5 = 2$，其中$T = 5$。$G_0$，$G_1$，...，$G_5$是多少？提示：反向工作。**
	
	$\begin{split}G_t = r_{t + 1} + \gamma G_{t + 1} & \Rightarrow G_{4} = 2 + 0.5 \times 0 = 2 \\ & \Rightarrow G_{3} = 3 + 0.5 \times 2 = 4 \\ & \Rightarrow G_{2} = 6 + 0.5 \times 4 = 8 \\ & \Rightarrow G_{1} = 2 + 0.5 \times 8 = 6 \\ & \Rightarrow G_{0} = 1 + 0.5 \times 6 = 4\end{split}$
8.  **假设$\gamma = 0.9$并且奖励序列是$R_1 = 2$，接着是无限序列的7s。$G_1$和$G_0$是什么？**
	
	$G_1 = 70$
	
	$G_t = r_{t + 1} + \gamma G_{t + 1} \Rightarrow G_0 = 2 + 0.9 \times 70 = 65$
9. **证明$\sum_{k = 0}^\infty\gamma^k = \frac1{1 - \gamma}$。**
10. **如果当前状态为$S_t$，并且根据随机策略$\pi$选择动作，则对于$\pi$和四参数函数$p$，$R_{t + 1}$的期望是多少？**
11. **用$q_\pi$和$\pi$给出$v_\pi$的等式。**
	
	$V_\pi(s) = \sum_{a \in A}\pi(a | s)Q_\pi(s, a)$
12. **根据$v_\pi$和四参数$p$给出$q_\pi$的等式。**
- **示例3.5：网格世界** 图（左图）显示了简单有限MDP的矩形网格世界表示。网格的单元格对应于环境的状态。在每个单元格中，可以有四个动作：北，南，东，西，这明确让个体在网格上的相应方向上移动一个单元格。使个体离开网格的操作会使其位置保持不变，但会导致-1的奖励。除了将个体从特殊状态$A$和$B$移出的行为，其他行为奖励值为$0$。在状态$A$，所有四个动作都会产生$+10$的奖励，并将个体送到$A'$ 。从状态$B$，所有动作都会获得$+5$的奖励，并将个体转到$B'$。
	 
	![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-3.2.png)
	
	**图** 网格世界的例子：等概率随机随机策略的特殊奖励动态（左）和状态值函数（右）。
13. **图（右）所示的值函数$v_\pi$，贝尔曼方程必须保持函数中的每个状态。以数字方式显示该等式的相对于其四个相邻状态，价值为$+2.3$，$+0.4$，$-0.4$和$+0.7$，保持中心状态，值为$+0.7$。（这些数字只能精确到一位小数。）**
	
	$V_\pi(s) = \sum_{a \in A}\pi(a | s)(R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)V_\pi(s')) \Rightarrow V_\pi(s) = \frac14 \times (0 + 1 \times 1 \times 2.3) + \frac14 \times (0 + 1 \times 1 \times 0.4) + \frac14 \times [0 + 1 \times 1 \times (-0.4)] + \frac14 \times (0 + 1 \times 1 \times 0.7) = 0.7$
14. **在网格世界的例子中，奖励对于目标是正的奖励，对于走进世界的边缘是负的惩罚，而在其余的时间里是零。这些奖励的符号是重要的吗，还是只是他们之间的间隔？请证明，使用$G_t = \sum_{k = 0}^\infty R_{t + k + 1}$， 向所有奖励添加常量$c$会使所有状态的价值增加一个常数$v_c$ ，因此不会影响任何策略下任何状态的相对值。$v_c$关于$c$和$\gamma$是什么？**
	
	$G_t = \sum_{k = 0}^\infty R_{t + k + 1} \Rightarrow G_t' = \sum_{k = 0}^\infty cR_{t + k + 1} = c\sum_{k = 0}^\infty R_{t + k + 1} = cG_t$
15. **现在考虑在情节任务，例如走迷宫中给所有的奖励增加一个常量$c$。这是否会有什么影响，还是会像以上那些连续任务那样保持不变？是或者否，为什么？举个例子。**
	
	是。$G_t = r_{t + 1} + \gamma r_{t + 2} + \dots + \gamma^{T - t - 1}r_T \Rightarrow G_t' = cr_{t + 1} + \gamma cr_{t + 2} + \dots + \gamma^{T - t - 1}cr_T = c(r_{t + 1} + \gamma r_{t + 2} + \dots + \gamma^{T - t - 1}r_T) = cG_t$
16. **动作价值，即$q_\pi$的贝尔曼方程是什么？必须根据状态—动作对$(s, a)$的可能后继的动作价值$q_\pi(s', a')$给出动作值$q_\pi(s, a)$。提示：对应于该方程的备用图在右图中给出。**
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/q_pi_backup_diagram.png)
	 
	$q\pi$的备份图
	
	$Q_\pi(s, a) = R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)\sum_{a' \in A}\pi(a', s')Q_\pi(s', a')$
17. **状态的值取决于在该状态下可能的动作的值以及当前策略下每个动作的可能性。我们可以根据状态的小的备份图来考虑这一点，并考虑到每个可能的操作：**
	 
	![](https://rl.qiwihui.com/zh-cn/latest/_images/exercise-3.18.png)
	
	**根据给定$S_t = s$的预期叶节点$q_\pi(s, a)$的值，给出对根节点$v_\pi(s)$的值的对应于该直觉和图表的方程。这个方程式应该包含一个符合策略$\pi$条件的预期。然后给出第二个等式，其中期望值以$\pi(a | s)$方式明确地写出，使得等式中不出现预期值符号。**
	
	$V_\pi(s) = \sum_{a \in A}\pi(a | s)Q_\pi(s, a)$
18. **动作值$q_\pi(s, a)$取决于预期的下一个奖励和剩余奖励的预期总和。再次，我们可以通过一个小的备份图来考虑这一点，这一个根源于一个动作（状态—动作对），并分支到可能的下一个状态：**
	 
	![](https://rl.qiwihui.com/zh-cn/latest/_images/exercise-3.19.png)
	
	**给定$S_t = s$和$A_t = a$，根据预期的下一个奖励$R_{t + 1}$和预期的下一个状态值$v_\pi(S_{t + 1})$，给出与这个直觉和图表对应的方程式的动作值$q_\pi(s, a)$。这个方程式应该包括期望值，但 _不包括_ 一个符合策略的条件。**
	
	$Q_\pi(s, a) = R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)V_\pi(s')$
19. **考虑右侧显示的持续MDP。唯一的决定是在顶点状态，有左 右两个动作可选。数字显示每次行动后收到的确定奖励。有两个确定性的策略，$\pi_左$和$\pi_右$。如果$\gamma = 0$，哪一种策略是最优的？如果$\gamma = 0.9$？如果$\gamma = 0.5$呢？**
	 
	![](https://rl.qiwihui.com/zh-cn/latest/_images/exercise-3.22.png)
	
	$\pi_左$，$\pi_左$，$\pi_右$
20. **给出回收机器人的$q^*$贝尔曼方程。**
	
	$Q^*(s, a) = R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)\max_{a'}Q^*(s', a')$
21. **网格世界问题的最优状态的最优价值为$24.4$，保留小数点后一位。以你对最优策略的了解和$G_t = \sum_{k = 0}^\infty R_{t + k + 1}$，以符号方式表示此值，然后将其计算为小数点后三位。**
	
	$V^*(s) = \max_a(R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)V^*(s')) = 10 + 0.9 \times 1 \times 16 = 24.4$
22. **用$q^*$给出$v^*$的方程。**
	
	$V^*(s) = \max_aQ^*(s, a)$
# 表格型方法
## 习题
1. **构成强化学习的马尔可夫决策过程的四元组有哪些变量？**
	
	状态、动作、状态转移概率和奖励，分别对应$(S, A, P, R)$，后面有可能会加上折扣因子构成五元组。
2. **请通俗地描述强化学习的“学习”流程。**
	
	可以将强化学习的“学习”流程类比于人类的学习流程。人类学习就是尝试每一条路，并记录尝试每一条路后的最终结果。在人类尝试的过程中，其实就可以慢慢地了解到哪一条路（对应于强化学习中的状态概念）会更好。我们用价值函数$V(s)$来定量表达该状态的优劣，然后用Q函数来判断在什么状态下做什么动作能够得到最大奖励，在强化学习中我们用Q函数来表示状态-动作值。
3. **请描述基于Sarsa算法的智能体的学习过程。**
	
	对于环境和智能体。两者每交互一次以后，智能体都会向环境输出动作，接着环境会反馈给智能体当前时刻的状态和奖励。那么智能体此时会进行两步操作：
	1. 使用已经训练好的Q表格，对应环境反馈的状态和奖励选取对应的动作进行输出。
	2. 我们已经拥有了$(s_t, a_t, r_{t + 1}, s_{t + 1}, a_{t + 1})$这几个值，并直接使用$a_{t + 1}$更新我们的Q表格。
4. **Q学习算法和Sarsa算法的区别是什么？**
	
	Sarsa算法是Q学习算法的改进（这句话可参考论文“On-Line Q-Learning Using Connectionist Systems”的摘要部分），详细描述如下。
	1. 首先，Q学习是异策略的时序差分学习方法，而Sarsa算法是同策略的时序差分学习方法。
	2. 其次，Sarsa算法在更新Q表格的时候所用到的$a'$是获取下一个Q值时一定会执行的动作。这个动作有可能是用$\epsilon$-贪心方法采样出来的，也有可能是$\max_Q$对应的动作，甚至是随机动作。
	3. 但是Q学习在更新Q表格的时候所用到的Q值$Q(S', a')$对应的动作不一定是下一步会执行的动作，因为下一步实际会执行的动作可能是因为进一步的探索而得到的。Q学习默认的动作不是通过行为策略来选取的，它默认$a'$为最佳策略对应的动作，所以Q学习算法在更新的时候，不需要传入$a'$，即$a_{t + 1}$。
	4. 更新公式的对比（区别只在目标计算部分）。
		
		Sarsa算法的公式：$r_{t + 1} + \gamma Q(s_{t + 1}, a_{t + 1})$ 。
		
		Q学习算法的公式：$r_{t + 1} + \gamma \max_aQ(s_{t + 1}, a)$ 。
	
	总结起来，Sarsa算法实际上是用固有的策略产生$S$，$A$，$R$，$S'$，$A'$这一条轨迹，然后使用$Q(s_{t + 1}, a_{t + 1})$更新原本的Q值$Q(s_t, a_t)$。但是Q学习算法并不需要知道实际上选择的动作，它默认下一个动作就是Q值最大的那个动作。所以Sarsa算法的动作通常会更加“保守胆小”，而对应的Q学习算法的动作会更加“莽撞激进”。
5. **同策略和异策略的区别是什么？**
	
	Sarsa算法就是一个典型的同策略算法，它只用一个$\pi$，为了兼顾探索和开发，它在训练的时候会显得有点儿“胆小怕事”。它在解决悬崖寻路问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心向未知区域探索了一些，也还是处在安全区域内，不至于掉入悬崖中。
	
	Q学习算法是一个比较典型的异策略算法，它有目标策略（target policy）用$\pi$来表示。此外还有行为策略（behavior policy），用$\mu$来表示。它分离了目标策略与行为策略，使得其可以大胆地用行为策略探索得到的经验轨迹来优化目标策略。这样智能体就更有可能探索到最优的策略。
	
	比较Q学习算法和Sarsa算法的更新公式可以发现，Sarsa算法并没有选取最大值的操作。因此，Q学习算法是非常激进的，其希望每一步都获得最大的奖励；Sarsa算法则相对来说偏保守，会选择一条相对安全的迭代路线。
## 面试题
1. **同学，你能否简述同策略和异策略的区别呢？**
	
	同策略和异策略的根本区别在于生成样本的策略和参数更新时的策略是否相同。对于同策略，行为策略和要优化的策略是同一策略，更新了策略后，就用该策略的最新版本对数据进行采样；对于异策略，其使用任意行为策略来对数据进行采样，并利用其更新目标策略。例如，Q学习在计算下一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优的动作，因此这里生成样本的策略和学习时的策略不同，所以Q学习算法是异策略算法；相对应的Sarsa算法则是基于当前的策略直接执行一次动作选择，然后用动作和对应的状态更新当前的策略，因此生成样本的策略和学习时的策略相同，所以Sarsa算法为同策略算法。
2. **能否细致地讲一下Q学习算法，最好可以写出其$Q(s, a)$的更新公式。另外，它是同策略还是异策略，原因是什么呢？**
	
	Q学习是通过计算最优动作价值函数来求策略的一种时序差分的学习方法，其更新公式为$$Q(s, a) \leftarrow Q(s, a) + \alpha(r(s, a) + \gamma\max_{a'}Q(s', a') - Q(s, a))$$其是异策略的，由于Q更新使用了下一个时刻的最大值，因此其只关心哪个动作使得$Q(s_{t + 1}, a)$取得最大值，而实际上到底采取了哪个动作（行为策略），Q学习并不关心。这表明优化策略并没有用到行为策略的数据，所以说它是异策略的。
3. **好的，看来你对于Q学习算法很了解，那么能否讲一下与Q学习算法类似的Sarsa算法呢，最好也可以写出其对应的$Q(s, a)$更新公式。另外，它是同策略还是异策略，为什么？**
	
	Sarsa算法可以算是Q学习算法的改进，其更新公式为$$Q(s, a) \leftarrow Q(s, a) + \alpha(r(s, a) + \gamma Q(s', a') - Q(s, a))$$其为同策略的，Sarsa算法必须执行两次动作得到$(s, a, r, s', a')$才可以更新一次；而且$a'$是在特定策略$\pi$的指导下执行的动作，因此估计出来的$Q(s, a)$是在该策略$\pi$下的Q值，样本生成用的$\pi$和估计的$\pi$是同一个，因此是同策略。
4. **请问基于价值的方法和基于策略的方法的区别是什么？**
	1. 生成策略上的差异，前者确定，后者随机。基于价值的方法中动作-价值对的估计值最终会收敛（通常是不同的数，可以转化为0～1的概率），因此通常会获得一个确定的策略；基于策略的方法不会收敛到一个确定的值，另外他们会趋向于生成最佳随机策略。如果最佳策略是确定的，那么最优动作对应的值函数的值将远大于次优动作对应的值函数的值，值函数的大小代表概率的大小。
	2. 动作空间是否连续，前者离散，后者连续。基于价值的方法，对于连续动作空间问题，虽然可以将动作空间离散化处理，但离散间距的选取不易确定。过大的离散间距会导致算法取不到最优动作，会在最优动作附近徘徊；过小的离散间距会使得动作的维度增大，会和高维度动作空间一样导致维度灾难，影响算法的速度。而基于策略的方法适用于连续的动作空间，在连续的动作空间中，可以不用计算每个动作的概率，而是通过正态分布选择动作。
	3. 基于价值的方法，例如Q学习算法，是通过求解最优价值函数而间接地求解最优策略；基于策略的方法，例如REINFORCE等算法直接将策略参数化，通过策略搜索、策略梯度或者进化方法来更新参数以最大化回报。基于价值的方法不易扩展到连续动作空间，并且当同时采用非线性近似、自举等策略时会有收敛问题。策略梯度具有良好的收敛性。
	4. 另外，对于价值迭代和策略迭代，策略迭代有两个循环，一个是在策略估计的时候，为了求当前策略的价值函数需要迭代很多次；另一个是外面的大循环，即策略评估、策略提升。价值迭代算法则是一步到位，直接估计最优价值函数，因此没有策略提升环节。
5. **请简述一下时序差分方法。**
	
	时序差分算法是使用广义策略迭代来更新Q函数的方法，核心是使用自举，即价值函数的更新使用下一个状态的价值函数来估计当前状态的价值。也就是使用下一步的Q值$Q(s_{t + 1}, a_{t + 1})$来更新当前步的Q值$Q(s_t, a_t)$。完整的计算公式如下：$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha(r_{t + 1} + \gamma Q(s_{t + 1}, a_{t + 1}))$$
7. **请问蒙特卡洛方法和时序差分方法是无偏估计吗？另外谁的方差更大呢？为什么？**
	
	蒙特卡洛方法是无偏估计，时序差分方法是有偏估计；蒙特卡洛方法的方差较大，时序差分方法的方差较小，原因在于时序差分方法中使用了自举，实现了基于平滑的效果，导致估计的价值函数的方差更小。
8. **能否简单说一下动态规划方法、蒙特卡洛方法和时序差分方法的异同点？**
	
	相同点：都用于进行价值函数的描述与更新，并且所有方法都基于对未来事件的展望计算一个回溯值。
	
	不同点：蒙特卡洛方法和时序差分方法属于免模型方法，而动态规划属于有模型方法；时序差分方法和蒙特卡洛方法，因为都是免模型的方法，所以对于后续状态的获知也都是基于试验的方法；时序差分方法和动态规划方法的策略评估，都能基于当前状态的下一步预测情况来得到对于当前状态的价值函数的更新。
	
	另外，时序差分方法不需要等到试验结束后才能进行当前状态的价值函数的计算与更新，而蒙特卡洛方法需要与环境交互，产生一整条马尔可夫链并直到最终状态才能进行更新。时序差分方法和动态规划方法的策略评估不同之处为免模型和有模型，动态规划方法可以凭借已知转移概率推断出后续的状态情况，而时序差分方法借助试验才能知道。
	
	蒙特卡洛方法和时序差分方法的不同在于，蒙特卡洛方法进行了完整的采样来获取长期的回报值，因而在价值估计上会有更小的偏差，但是也正因为收集了完整的信息，所以价值的方差会更大，原因在于其基于试验的采样得到，和真实的分布有差距，不充足的交互导致较大方差。而时序差分方法则相反，因为它只考虑了前一步的回报值，其他都是基于之前的估计值，因而其价值估计相对来说具有偏差大方差小的特点。
	
	三者的联系：对于$\mathrm{TD}(\lambda)$方法，如果$\lambda = 0$，那么此时等价于时序差分方法，即只考虑下一个状态；如果$\lambda = 1$，等价于蒙特卡洛方法，即考虑$T - 1$个后续状态直到整个试验结束。
# 动态规划
## 练习
- **例：赌徒问题** 一个赌徒对掷硬币的游戏进行下注。如果硬币正面朝上，他将赢得押在这一掷上的所有赌注，如果是反面朝上，他将输掉所押的赌注。如果赌徒赢得100美元或者输光了钱那么游戏结束。每一次掷硬币，赌徒要决定押多少钱，这些钱必须是整数美元。这个问题可以被建模为一个无衰减，回合式的有限MDP。状态是赌徒的资本，$s \in \{1, 2, \dots, 99\}$，动作是押注多少，$a \in \{0, 1, \dots, \min(s, 100- s)\}$。赌徒达到目标时奖励是+1，其他转移过程都为0。状态价值函数给出了从每个状态出发能够赢得的概率。策略是从资本多少到押注的一个映射。最优策略最大化达到目标的概率。$p_h$代表硬币正面朝上的概率。如果$p_h$知道了，那么整个问题都清楚了，并且可以被解决，比如用价值迭代方法。图展示出了价值函数经过成功的价值迭代更新后的变化，并且找到了$p_h = 0.4$情况下最终的策略。这个策略是最优的，但不是唯一的。实际上，有很多最优策略，都取决于相对于最优价值函数选取的$\argmax$动作。你能想象出所有的最优策略是什么样的吗？
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-4.3.png)
	
	**图：**$p_h = 0.4$情况下赌徒问题的解。上面的图是经过价值迭代成功地更新找到的价值函数。下面是最终的策略。
1. **为什么赌徒问题的最优策略有这样奇怪的形式？尤其是，在资本剩余50美元的时候需要押注所有在一次投掷上，但是对于资本剩余51美元的时候却没有这样做。为什么这会是一个好的策略？**
# 蒙特卡洛方法
## 练习
- **例：二十一点（Blackjack）** 这个风靡于赌场的游戏 _二十一点_ 是一种比谁手上的牌点数和大（不超过21点）的游戏。其中，所有的J，Q，K记为10点，A即可为1也可为11。我们考虑的版本是玩家独立面对庄家。游戏开始时每人手上有两张牌，其中庄家的牌有一张是明牌。如果玩家起手就是21点（一张10点的牌和一张A），这个叫 _natural_ 。这种情况下就算庄家也是natural也判玩家赢，这种情况叫draw，游戏结束。如果玩家起手点数小于21点，那么他可以要牌，每次要一张，或者停止要牌。如果玩家的牌超过21点，玩家就直接输了，叫做爆掉；如果玩家停止要牌，那么进入庄家回合。庄家要牌或者停止要牌根据的策略是，只要超过17点就停止，否则就一直要牌，没有其他选择。如果庄家爆掉，那么玩家赢。如果大家都没爆掉，那么就比谁的点数接近21点。
	
	21点可以看成一个回合式的有限马尔可夫过程。每次游戏都是一个回合。赢、输、draw的奖励分别为1、-1、0。游戏过程中的任意动作奖励都为0，且我们不算折扣（$\gamma = 1$）；因此这些结束状态的奖励即是回报。玩家的动作只有要牌或者停止要牌两种。游戏的状态取决于玩家是什么牌以及庄家手上的明牌。我们假设牌数是无限的，这样我们记牌就没有优势了。如果玩家将A当成11点来算的话（不能爆掉），我们称它 _可用_ 。 开始时我们把A当成11，因为如果当成1的话，那么开始时的牌肯定是小于等于11点的，这样我们没有更多的选择，肯定是会要牌的。所以，玩家做出的决定依赖于三个变量：当前的牌的点数和（12-21）、庄家的明牌的点数（A-10）以及玩家是否有可用的A。这样的话，总共有200个不同的状态。
	
	我们考虑这样的策略：一直要牌，直到点数和等于20或21时停止。为了使用蒙特卡洛方法找到这个策略下的状态价值函数，我们使用一个模拟器模拟了许多次的游戏，游戏中玩家使用上述的策略。然后我们将每个状态的回报值求平均，作为对应状态的价值函数。通过这种方法求得的价值函数如图所示。可以看到，如果A可用，相对于A不可用，估计的值会有更多不确定性，更加不规则，因为这些状态不是很常见。经过500000次的游戏，我们看到价值函数被近似得很好。
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-5.1.png)
	
	图：遵循一直要牌直到点数和等于20或21的策略，使用蒙特卡洛策略估计求得的估计的状态价值函数。
1. **请思考，图右边两幅图：为什么估计值在尾部的最后两行会突然变大？为什么最左边的整个一行价值会有下降？为什么前面部分，上面图中的价值要大于下面的图？**
2. **假设在二十一点任务中使用每次访问MC而不是首次访问MC。你是否期望结果会有很大差异？为什么或者为什么不？**

	否。
3. **考虑具有单个非终结状态的MDP和单个动作，该动作以概率$p$转换回非终结状态并以概率$1 - p$转换到终端状态。令在所有过渡中奖励都是$+1$，并且令$\gamma = 1$。假设您观察到一个持续10个步骤的事件，回报为10。非终结状态值的首次访问和每次访问估算是什多少？**
4. **给定策略$b$的回报，式$V(s) = \frac{\sum_{t \in \mathcal T(s)}\rho_{t:T(t) - 1}G_t}{\sum_{t \in \mathcal T(s)}\rho_{t:T(t) - 1}}$中状态价值$V(s)$换成 _动作_ 价值$Q(s, a)$的表达式是什么？**
- **例：离策略估计21点的状态价值** 我们应用两种重要性采样方法，从离策略的数据估计单个21点状态的价值。前面讲过，蒙特卡洛方法的一个优势是它可以用来估计单一的一个状态，不用生成其他状态的估计。这个例子中，我们估计庄家是两点，玩家点数和是13点，玩家有一个使用的A（即玩家有A和2两张牌）。从这个状态生成数据，然后以相同的概率选择要牌或停止（行为策略）。目标策略是只有当点数和为20或21时才停止。目标策略的价值大概是$-0.27726$（这是由目标策略生成一百万个回合的回报求平均而得）。两种离策略方法在$1000$个随机的离策略回合后，估计的价值很接近这个值了。为使我们的结果更可信，我们独立地进行了$100$次实验，每次估计值都从零开始，学习$10000$个回合。图显示了学习曲线——两种方法各自的均方误差是回合数的函数，结果是$100$次实验的平均。两种算法的误差都趋向于零，但是加权重要性采样在开始的时候误差更小，这在实践中很典型。
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-5.3.png)
	
	**图：** 从离策略回合数据估计21点的单个状态的价值，加权重要性采样有更低的估计误差。
5. **对原始重要性采样方法而言，像图那样，学习曲线中误差是随着训练次数的增加而减少的。 但是，对加权重要性采样，误差是先增加然后减少，你如何看待这种现象？**
- **例：无限方差** 对原始重要性采样的估计通常会有无限的方差，因此带来了不太让人满意的收敛特性，即无论合适，缩放的回报都有无限的方差——而这在回合的轨迹中包含环时更加容易发生。一个简单的例子如图所示。这里只有一个非结束状态$s$和两个动作，**结束**和**返回**。**结束**动作会百分百导致回合结束，而**返回**动作会有$0.9$的可能返回状态$s$，有$0.1$的可能到结束状态。返回动作导致结束的话，有$+1$的奖励；返回状态$s$的话，奖励为零。考虑目标策略是一直选择**返回**的动作。所有的回合都包含了数次（可能是零次）返回状态$s$，然后到结束，并获得奖励，回合的回报为$+1$。因此，在目标策略下，状态$s$的价值是$1$（$\gamma = 1$）。假设我们使用行为策略生成的离策略数据来估计这个状态的价值，该行为策略选择以等概率随机地选择**结束**和**返回**两种动作。
	 
	![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-5.4.png)
	
	**图：** 原始重要性采样估计例的单状态MDP，产生了惊人的不稳定性。正确的估计值应该是1（$\gamma = 1$），即使只有一次回报（在重要性采样之后）。但图中样本的方差是无限的，估计值不能收敛于这个正确值。这些结果对应于离策略首次访问MC方法。
6. **写出从$\rho_{t:T - 1}R_{t + 1} = \frac{\pi(A_t | S_t)}{b(A_t | S_t)}\frac{\pi(A_{t + 1} | S_{t + 1})}{b(A_{t + 1} | S_{t + 1})}\frac{\pi(A_{t + 2} | S_{t + 2})}{b(A_{t + 2} | S_{t + 2})}\dots\frac{\pi(A_{T - 1} | S_{T - 1})}{b(A_{T - 1} | S_{T - 1})}R_{t + 1}$导出$\mathbb E[\rho_{t:T - 1}R_{t + 1}] = \mathbb E[\rho_{t:t}R_{t + 1}]$的步骤。**
	
	$\rho_{t:T - 1}R_{t + 1} = \frac{\pi(A_t | S_t)}{b(A_t | S_t)}\frac{\pi(A_{t + 1} | S_{t + 1})}{b(A_{t + 1} | S_{t + 1})}\frac{\pi(A_{t + 2} | S_{t + 2})}{b(A_{t + 2} | S_{t + 2})}\dots\frac{\pi(A_{T - 1} | S_{T - 1})}{b(A_{T - 1} | S_{T - 1})}R_{t + 1} \Rightarrow \mathbb E[\rho_{t:T - 1}R_{t + 1}] = \mathbb E[\frac{\pi(A_t | S_t)}{b(A_t | S_t)}\frac{\pi(A_{t + 1} | S_{t + 1})}{b(A_{t + 1} | S_{t + 1})}\frac{\pi(A_{t + 2} | S_{t + 2})}{b(A_{t + 2} | S_{t + 2})}\dots\frac{\pi(A_{T - 1} | S_{T - 1})}{b(A_{T - 1} | S_{T - 1})}R_{t + 1}] = \mathbb E[\frac{\pi(A_t | S_t)}{b(A_t | S_t)}R_{t + 1}]\mathbb E[\frac{\pi(A_{t + 1} | S_{t + 1})}{b(A_{t + 1} | S_{t + 1})}]\mathbb E[\frac{\pi(A_{t + 2} | S_{t + 2})}{b(A_{t + 2} | S_{t + 2})}]\dots\mathbb E[\frac{\pi(A_{T - 1} | S_{T - 1})}{b(A_{T - 1} | S_{T - 1})}] = \mathbb E[\frac{\pi(A_t | S_t)}{b(A_t | S_t)}R_{t + 1}] \times 1 \times 1 \times \dots \times 1 = \mathbb E[\frac{\pi(A_t | S_t)}{b(A_t | S_t)}R_{t + 1}] = E[\rho_{t:t}R_{t + 1}]$
# 时序差分学习
## 练习
- **例 开车回家** 每天下班回家后，你都会尝试预测回家需要多长时间。当你离开办公室时，你会记下时间，星期几，天气以及其他可能相关的内容。这个星期五你正好在6点钟离开，你估计要回家需要30分钟。当你到达你的车是6:05，你注意到开始下雨了。在雨中交通通常较慢，所以你需要花费35分钟，或者总共40分钟。十五分钟后，你及时完成了旅程的高速公路部分。当你驶出高速进入第二部分道路时，你将总旅行时间的估计值减少到35分钟。不幸的是，此时你被困在一辆慢卡车后面，而且道路太窄而无法通过。你最终不得不跟随卡车，直到6:40你转到住的小街。三分钟后你就回家了。因此，状态，时间和预测的顺序如下：
	
	| **状态** | **经过时间（分钟）** | **预测到的时间** | **预计总时间** |
	| --- | --- | --- | --- |
	| 周五6点离开办公室 | 0 | 30 | 30 |
	| 到达车，下雨 | 5 | 35 | 40 |
	| 驶出高速公路 | 20 | 15 | 35 |
	| 第二条路，在卡车后面 | 30 | 10 | 40 |
	| 进入家的街道 | 40 | 3 | 43 |
	| 到家 | 43 | 0 | 43 |
	
	这个例子中的奖励是旅程每一段的经过时间。我们不打折（$\gamma = 1$），因此每个状态的回报是从该状态开始的实际时间。每个状态的价值是 _预期的_ 时间。第二列数字给出了遇到的每个状态的当前估计值。
	
	查看蒙特卡罗方法操作的一种简单方法是绘制序列上预测的总时间（最后一列），如图（左）所示。红色箭头表示常量-$\alpha$ MC方法推荐的预测变化，其中$\alpha = 1$。这些正是每个状态的估计值（预测的行走时间）与实际返回（实际时间）之间的误差。例如，当你离开高速公路时，你认为回家仅需15分钟，但实际上需要23分钟。公式$V(s_t) \leftarrow V(s_t) + \alpha(G_{i, t} - V(s_t))$适用于此点，并确定驶出公路后的估计时间的增量。误差$G_t - V(S_t)$此时为8分钟。假设步长参数$\alpha$为1/2。然后，由于这种经验，退出高速公路后的预计时间将向上修改四分钟。在这种情况下，这可能是一个太大的变化；卡车可能只是一个不幸的中断。无论如何，只有在你到家之后才能进行变更。只有在这一点上你才知道任何实际的回报。
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-6.1.png)
	
	**图** 通过蒙特卡罗方法（左）和TD方法（右）在开车回家示例中推荐的变化。
1. **这是一个练习，以帮助你发展直觉，了解为什么TD方法通常比蒙特卡罗方法更有效。考虑开车回家示例以及如何通过TD和蒙特卡罗方法解决它。你能想象一个TD更新平均比蒙特卡罗更新更好的情景吗？ 举一个示例场景 - 过去经验和当前状态的描述 - 你期望TD更新更好。这里有一个提示：假设你有很多下班开车回家的经验。 然后你搬到一个新的建筑物和一个新的停车场（但你仍然在同一个地方进入高速公路）。现在你开始学习新建筑的预测。 在这种情况下，你能看出为什么TD更新可能会好得多，至少初始是这样吗？在原始场景中发生同样的事情可能吗？**
- **例 随机行走**
	
	在这个例子中，我们在应用于以下马尔可夫奖励过程时，凭经验比较TD(0)和常数-$\alpha$ MC的预测能力：
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/random_walk_markov_reward_process.png)
	
	_马尔可夫奖励过程_（MRP）是没有行动的马尔可夫决策过程。我们经常在关注预测问题时使用MRP，其中不需要将由环境引起的动态与由个体引起的动态区分开来。在该MRP中，所有回合以中心状态$C$开始，然后以相同的概率在每一步上向左或向右前进一个状态。回合终止于最左侧或最右侧。当回合在右边终止时，会产生$+1$的奖励；所有其他奖励都是零。例如，典型的回合可能包含以下状态和奖励序列：$C$，$0$，$B$，$0$，$C$，$0$，$D$，$0$，$E$，$1$。因为此任务是未折扣的，所以每个状态的真实价值是从该状态开始在右侧终止的概率。因此，中心状态的真值是$v_\pi(C) = 0.5$。所有状态$A$到$E$的真实价值都是$\frac16$，$\frac26$，$\frac36$，$\frac46$和$\frac56$。
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/random_walk_comparison.png)
	
	上面的左图显示了在$TD(0)$的单次运行中在不同数量的回合之后学习的价值。100回合之后的估计值与它们的真实值接近 - 具有恒定的步长参数（在此示例中$\alpha = 0.1$），这些值随着最近一个回合的结果而无限地波动。右图显示了两种方法对于各种$\alpha$值的学习曲线。显示的性能度量是学习的值函数和真值函数之间的均方根（RMS）误差，在五个状态上取平均值，然后在超过100次运行上平均。在所有情况下，对于所有$s$，近似值函数被初始化为中间值$V(s) = 0.5$。在这项任务中，TD方法始终优于MC方法。
2. **从随机游走示例的左图中显示的结果看来，第一回合仅导致$V(A)$的变化。这告诉你第一回合发生了什么？为什么只有这一状态的估计值发生了变化？确切地说它改变了多少？**
3. **随机游走示例右图中显示的特定结果取决于步长参数$\alpha$的值。如果使用更广范围的$\alpha$值，您认为关于哪种算法更好的结论是否会受到影响？是否存在不同的固定值$\alpha$，其中任何一种算法的表现都要比显示的好得多？为什么或者为什么不？**
4. **在随机游走示例的右图中，TD方法的RMS误差似乎下降然后再上升，特别是在$\alpha$高时。可能是什么导致了这个？你认为这总是会发生，或者它可能是近似值函数初始化的函数吗？**
5. **在例中，我们说状态$A$到$E$随机游走示例的真实值是$\frac16$，$\frac26$，$\frac36$，$\frac46$和$\frac56$。描述至少两种不同的方式相说明这些可以计算出来。您认为我们实际使用哪个？为什么？**
6. **为什么Q-learning被认为是一种 _离策略_ 控制方法？**
7. **假设动作选择是贪婪的。Q-learning与Sarsa的算法完全相同吗？他们会做出完全相同的动作选择和权重更新吗？**
	
	不。
# $n$步引导（Bootstrapping）方法
- **例：随机行走的n步TD方法** 考虑5个状态随机游走任务中使用n步TD方法。假设第一回合直接从中心状态 **C** 向右进行，通过 **D** 和 **E**，然后在右边终止，回报为1。回想所有状态的估计值从中间值开始，$V(s) = 0.5$。作为这种经验的结果，一步法只改变最后一个状态$V(E)$的估计值，该值将增加到1，观察到的返回值。另一方面，两步法将增加终止前两个状态的值：$V(D)$和$V(E)$都将增加到1。三步法或任何n步法（对$n \ge 2$），将所有三个访问状态的值增加到1，全部增加相同的量。
	 
	哪个n的值更好？图显示了对较大随机行走过程进行简单经验测试的结果，其中包含19个状态而不是5个（从左侧离开回报为−1，所有值都初始化为0），我们在本章中将其用作运行示例。结果显示了涉及大范围n和$\alpha$的值的n步TD方法。垂直轴上显示的每个参数设置的性能度量是19个状态的回合结束时，预测与其真实值之间的均方误差的平方根，然后在整个实验的前10回合和100次重复中取平均值（所有参数设置都使用相同的行走集合）。请注意，中间值为n的方法效果最好。这说明了TD和蒙特卡罗方法对n步方法的推广能够比两种极端方法中的任何一种方法表现更好。
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-7.2.png)
	
	**图：** 对于19个状态的随机行走任务的不同n值，n步TD方法的性能是$\alpha$的函数（例）。
1. **为什么你认为在本章的例子中使用了更大的随机游走任务（19个州而不是5个）？较小的步行会将优势转移到不同的n值吗？在较大的步行中左侧结果从0变为−1是怎么发生的？你认为这对n的最佳价值有任何不同吗？**
	
	会。有。
# 表格方法规划和学习
图显示了规划个体发现解决方案的速度比非规划个体快得多的原因。显示了在第二回合中途由$n = 0$和$n = 50$个体发现的策略。如果没有规划（$n = 0$），每回合只会为策略添加一个额外的步骤，因此到目前为止只学习了一步（最后一步）。通过规划，在第一回合中再次只学习一步，但是在第二回合中，已经开发了一个广泛的策略，在该回合结束时将几乎回到开始状态。此策略由规划过程构建，而个体仍然在启动状态附近徘徊。到第三回合结束时，将找到完整的最优政策并获得完美的表现。

![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-8.3.png)

**图：** 在第二回合中途通过规划和非规划Dyna-Q个体找到的策略。箭头表示每个状态的贪婪行为；如果没有显示状态的箭头，则其所有动作价值都相等。黑色方块表示个体的位置。
1. **图中的非规划方法看起来特别差，因为它是一步法；使用多步引导的方法会做得更好。你认为多步引导方法之一可以和Dyna方法一样吗？解释为什么能或者为什么不能。**
	
	能。
# 策略梯度
## 习题
1. **如果我们想让机器人自己玩视频游戏，那么强化学习中的3个组成部分（演员、环境、奖励函数）具体分别代表什么？**
	
	演员做的事情就是操控游戏的摇杆，比如向左、向右、开火等操作；环境就是游戏的主机，负责控制游戏的画面、控制怪物如何移动等；奖励函数就是当执行什么动作、发生什么状况的时候，我们可以得到多少分数，比如击杀一只怪兽得到20分、被对手暴击扣除10分、完成任务得到10分等。
2. **在一个过程中，一个具体的轨迹$s_1$，$a_1$，$s_2$，$a_2$出现的概率取决于什么？**
	1. 一部分是环境的行为，即环境的函数内部的参数或内部的规则是什么形式的。$p(s_{t+  1} | s_t, a_t)$这一项代表的是环境，环境这一项通常是无法控制的，因为它是已经客观存在的，或者其形式是提前制定好的。
	2. 另一部分是智能体的行为，我们能控制的是$p_\theta(a_t | s_t)$ ，即给定一个状态$s_t$，演员要采取什么样的动作$a_t$取决于演员的参数$\theta$，所以这部分是我们可以控制的。随着演员动作的不同，每个同样的轨迹，它会因为不同的概率从而表现出不同的行为。
3. **当我们最大化期望奖励时，应该使用什么方法？**
	
	应该使用梯度上升法，因为要让期望奖励越大越好，所以是梯度上升法。梯度上升法在更新参数的时候要添加梯度信息。要进行梯度上升，我们先要计算期望奖励$\bar R$的梯度。我们对$\bar R$取一个梯度，这里只有$p_\theta(\tau)$是与$\theta$有关的，所以$p_\theta(\tau)$为梯度的部分。
4. **我们应该如何理解策略梯度的公式呢？**
	
	策略梯度的公式如下：
	$$E_{\tau \sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)] \approx \frac1N\sum_{n = 1}^NR(\tau^n)\nabla\log p_\theta(\tau^n) = \frac1N\sum_{n = 1}^N\sum_{t = 1}^{T_n}R(\tau^n)\nabla\log p_\theta(a_t^n | s_t^n)$$
	$p_\theta(\tau)$里面有两项，$p(s_{t + 1} | s_t, a_t)$来自环境，$p_\theta(a_t | s_t)$来自智能体。$p(s_{t + 1} | s_t, a_t)$由环境决定，从而与$\theta$无关，因此$\nabla\log p(s_{t + 1} | s_t, a_t) = 0$，$\nabla p_\theta(\tau) = \nabla\log p_\theta(a_t^n |s_t^n)$。
	
	具体来说：
	1. 假设在状态$s_t$时执行动作$a_t$，最后发现轨迹$\tau$的奖励是正的，那我们就要增大这一项的概率，即增大在状态$s_t$时执行动作$a_t$的概率；
	2. 反之，在状态$s_t$时执行动作$a_t$会导致轨迹$\tau$的奖励变成负的，我们就要减小这一项的概率。
5. **我们可以使用哪些方法来进行梯度提升的计算？**
	
	用梯度提升来更新参数，对于原来的参数$\theta$，可以将原始的$\theta$加上更新梯度，再乘一个学习率。通常学习率也需要调整，与神经网络一样，我们可以使用Adam、RMSProp、SGD等优化器对其进行调整。
6. **进行基于策略梯度的优化的技巧有哪些？**
	1. 增加基线：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一项变换的概率上升，我们把奖励减去一项$b$，称之为基线。当减去$b$后，就可以让奖励$R(\tau^n) − b$有正有负。所以如果得到的总奖励$R(\tau^n)$大于$b$，就让它的概率增大。如果总奖励小于$b$，就算它是正的，值很小也是不好的，就需要让这一项的概率减小。如果奖励$R(\tau^n)$小于$b$ ，就要让采取这个动作的奖励下降，这样也符合常理。但是使用基线会让本来奖励很大的“动作”的奖励变小，从而降低更新速率。
	2. 指派合适的分数：首先，原始权重是整个回合的总奖励。现在改成从某个时间点$t$开始，假设动作是在时间点$t$被执行的，从时间点$t$，一直到游戏结束所有奖励的总和大小，才真正代表这个动作是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，我们称由此得到的奖励的和为折扣回报。
	3. 综合以上两种技巧，我们将其统称为优势函数，用$A$来代表优势函数。优势函数取决于状态和动作，即我们需计算的是在某一个状态$s$采取某一个动作$a$的时候，优势函数有多大。
	4. 优势函数的意义在于衡量假设我们在某一个状态$s_t$执行某一个动作$a_t$，相较于其他可能动作的优势。它在意的不是绝对的好，而是相对的好，即相对优势，因为会减去一个基线$b$。$A_\theta(s_t, a_t)$通常可以由一个网络预估出来，这个网络叫作评论员。
7. **对于策略梯度的两种方法，蒙特卡洛强化学习和时序差分强化学习两种方法有什么联系和区别？**
	1. 两者的更新频率不同。蒙特卡洛强化学习方法是每一个回合更新一次，即需要经历完整的状态序列后再更新，比如贪吃蛇游戏，贪吃蛇“死了”即游戏结束后再更新。而时序差分强化学习方法是每一步就更新一次，比如贪吃蛇游戏，贪吃蛇每移动一次（或几次）就进行更新。相对来说，时序差分强化学习方法比蒙特卡洛强化学习方法更新的频率更高。
	2. 时序差分强化学习方法能够在知道一个小步后就进行学习，相比于蒙特卡洛强化学习方法，其更加快速和灵活。
	3. 具体例如：假如我们要优化开车去公司的通勤时间。对于此问题，每一次通勤，我们将到达不同的路口。对于时序差分强化学习方法，其会对每一个经过的路口计算时间，例如在路口A就开始更新预计到达路口B、路口C…… ，以及到达公司的时间；对于蒙特卡洛强化学习方法，其不会每经过一个路口就更新时间，而是到达最终的目的地后，再修改到达每一个路口和到达公司对应的时间。
8. **请详细描述REINFORCE算法的计算过程。**
	
	首先我们需要根据一个确定好的策略模型来输出每一个可能动作的概率，对于所有动作的概率，我们使用采样方法（或者是随机的方法）选择一个动作与环境进行交互，同时环境会给我们反馈整个回合的数据。将此回合数据输入学习函数中，并根据回合数据进行损失函数的构造，通过Adam等优化器的优化，再更新我们的策略模型。
## 面试题
1. **可以说一下你所了解的基于策略梯度优化的技巧吗？**
	1. 增加基线：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一个变换的概率上升，我们把奖励减去一项$b$，称$b$为基线。当减去$b$以后，就可以让奖励$R(\tau^n) − b$有正有负。如果得到的总奖励$R(\tau^n)$大于$b$，就让它的概率上升。如果总奖励小于$b$，就算它是正的，值很小也是不好的，就需要让它的概率下降。如果总奖励小于$b$，就要让采取这个动作的奖励下降，这样也符合常理。但是使用基线会让本来奖励很大的“动作”的奖励变小，降低更新速率。
	2. 指派合适的分数：首先，原始权重是整个回合的总奖励。现在改成从某个时间点$t$开始，假设这个动作是在时间点$t$被执行的，那么从时间点$t$，一直到游戏结束所有奖励的总和，才真的代表这个动作是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，这里我们称由此得到的奖励的和为折扣回报。
	3. 综合以上两种技巧，我们将其统称为优势函数，用$A$来代表优势函数。优势函数取决于状态和动作，即我们需计算的是在某一个状态$s$采取某一个动作$a$的时候，优势函数有多大。
## 练习
- **例：带有切换动作的短廊**
	
	考虑下图所示的小走廊网格世界。与往常一样，奖励为每步$-1$。在这三个非终结状态的每一个中，只有两个动作，即**向右**和**向左**。这些动作在第一状态和第三状态中具有通常的结果（在第一状态中**向左**则不移动），但是在第二状态中，它们是相反的，因此右移**向左**，左移**向右**。 问题是困难的，因为在函数近似下所有状态看起来都相同。特别地，我们为所有$s$定义$x(s, \mathrm{right}) = [1,0]^\top$和$x(s, \mathrm{left}) = [0,1]^\top$。具有$\epsilon$贪婪动作选择的动作值方法被迫在两个策略之间进行选择：在所有步骤上以高概率$1 - \epsilon / 2$选择**向右**或在所有时步上以相同的高概率选择左。如果$\epsilon = 0.1$，则这两个策略达到的值（在开始状态下）分别小于$-44$和$-82$，如图所示。如果一种方法可以学习选择**向右**的特定概率，则可以做得更好。最佳概率约为0.59，最后值大约为$-11.6$。
	
	![](https://rl.qiwihui.com/zh-cn/latest/_images/example-13.1.png)
1. **用你对网格世界及其动力学的知识来确定一个精确的符号表达式，以获取示例中选择向右动作的最佳概率。**
# 资格迹
## 练习
1. **正如回报可以按照第一个奖励和一步之后递归写入，同样$\lambda$回报也可以。从$G_t^\lambda = (1 - \lambda)\sum_{n = 1}^\infty\lambda^{n - 1}G_{t:t + n}$和$G_{t:t + n} = R_{t + 1} + \gamma R_{t + 2} + \dots + \gamma^{n - 1} R_{t + n} + \gamma^n\hat v(S_{t + n}, \mathbf w_{t + n - 1}), 0 \le t \le T - n$得出类似的递归关系。**
	
	$G_t^\lambda = (1 - \lambda)G_t + \lambda G_{t + 1}^\lambda$

![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-12.2.png)

**图：** 每个$n$步回报的$\lambda$回报给出的加权。
2. **该参数$\lambda$表征图中的指数加权下降的速度，以及因此$\lambda$回报算法在确定其更新时所看到的未来的距离。但是速率因素比如$\lambda$有时是表征衰变速度的尴尬方式。出于某些目的，最好指定时间常数或半衰期。什么是与$\lambda$和半衰期$\tau_\lambda$，即加权序列将落到其初始值的一半的时间相关的等式？**