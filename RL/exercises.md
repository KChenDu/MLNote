- [绪论](#绪论)
- [多臂赌博机问题](#多臂赌博机问题)
- [马尔可夫决策过程](#马尔可夫决策过程)
# 绪论
## 习题
1. **强化学习的基本结构是什么？**
	
	本质上是智能体与环境的交互。具体地，当智能体在环境中得到当前时刻的状态后，其会基于此状态输出一个动作，这个动作会在环境中被执行并输出下一个状态和当前的这个动作得到的奖励。智能体在环境里存在的目标是最大化期望累积奖励。
2. **强化学习相对于监督学习为什么训练过程会更加困难？**
	1. 强化学习处理的大多是序列数据，其很难像监督学习的样本一样满足独立同分布条件。
	2. 强化学习有奖励的延迟，即智能体的动作作用在环境中时，环境对于智能体状态的奖励存在延迟，使得反馈不实时。
	3. 监督学习有正确的标签，模型可以通过标签修正自己的预测来更新模型，而强化学习相当于一个“试错”的过程，其完全根据环境的“反馈”更新对自己最有利的动作。
3. **强化学习的基本特征有哪些？**
	1. 有试错探索过程，即需要通过探索环境来获取对当前环境的理解。
	2. 强化学习中的智能体会从环境中获得延迟奖励。
	3. 强化学习的训练过程中时间非常重要，因为数据都是时间关联的，而不是像监督学习中的数据大部分是满足独立同分布的。
	4. 强化学习中智能体的动作会影响它从环境中得到的反馈。
4. **近几年强化学习发展迅速的原因有哪些？**
	1. 算力的提升使我们可以更快地通过试错等方法来使得智能体在环境里面获得更多的信息，从而取得更大的奖励。
	2. 我们有了深度强化学习这样一个端到端的训练方法，可以把特征提取、价值估计以及决策部分一起优化，这样就可以得到一个更强的决策网络。
5. **状态和观测有什么关系？**
	
	状态是对环境的完整描述，不会隐藏环境信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用同一个实值向量、矩阵或者更高阶的张量来表示状态和观测。
6. **一个强化学习智能体由什么组成？**
	1. 策略函数，智能体会用策略函数来选取它下一步的动作，策略包括随机性策略和确定性策略。
	2. 价值函数，我们用价值函数来对当前状态进行评估，即进入现在的状态可以对后面的奖励带来多大的影响。价值函数的值越大，说明进入该状态越有利。
	3. 模型，其表示智能体对当前环境状态的理解，它决定系统是如何运行的。
7. **根据强化学习智能体的不同，我们可以将其分为哪几类？**
	1. 基于价值的智能体。显式学习的是价值函数，隐式地学习智能体的策略。因为这个策略是从学到的价值函数里面推算出来的
	2. 基于策略的智能体。其直接学习策略，即直接给智能体一个状态，它就会输出对应动作的概率。当然在基于策略的智能体里面并没有去学习智能体的价值函数。
	3. 另外还有一种智能体，它把以上两者结合。把基于价值和基于策略的智能体结合起来就有了演员-评论员智能体。这一类智能体通过学习策略函数和价值函数以及两者的交互得到更佳的状态。
8. **基于策略迭代和基于价值迭代的强化学习方法有什么区别？**
	1. 基于策略迭代的强化学习方法，智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据该策略进行操作。强化学习算法直接对策略进行优化，使得制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。
	2. 基于价值迭代的方法只能应用在离散的环境下，例如围棋或某些游戏领域，对于行为集合规模庞大或是动作连续的场景，如机器人控制领域，其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。
	3. 基于价值迭代的强化学习算法有Q-learning、Sarsa等，基于策略迭代的强化学习算法有策略梯度算法等。
	4. 此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，从而取得更好的效果。
9. **有模型学习和免模型学习有什么区别？**
	
	针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。总体来说，有模型学习相比免模型学习仅仅多出一个步骤，即对真实环境进行建模。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。
10. **如何通俗理解强化学习？**
	
	环境和奖励函数不是我们可以控制的，两者是在开始学习之前就已经事先确定的。我们唯一能做的事情是调整策略，使得智能体可以在环境中得到最大的奖励。另外，策略决定了智能体的行为，策略就是给一个外界的输入，然后它会输出现在应该要执行的动作。
## 面试题
1. **看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？**
	
	强化学习包含环境、动作和奖励3部分，其本质是智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大，或者说是期望最大。
2. **请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？**
	
	首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建和训练。其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。另外，强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独立同分布条件。
3. **根据你的理解，你认为强化学习的使用场景有哪些呢？**
	
	7个字总结就是“多序列决策问题”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达的条件，即满足可学习条件。
4. **请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？**
	
	深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大。
5. **你了解有模型和免模型吗？两者具体有什么区别呢？**
	
	我认为两者的区别主要在于是否需要对真实的环境进行建模，免模型方法不需要对环境进行建模，直接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如果建模的环境与真实环境的差异较大，那么会限制其泛化性能。现在通常使用有模型方法进行模型的构建工作。
# 多臂赌博机问题
## 练习
1. **在$\epsilon$贪婪动作选择中，对于两个动作的情况和$\epsilon = 0.5$，选择贪婪动作的概率是多少？**
	
	$1 - \epsilon + \epsilon\times\frac12 = 1 - 0.5 + 0.5\times\frac12 = \frac34$
2. **赌博机示例 考虑具有$k = 4$动作的$k$臂赌博机问题，表示为1，2，3和4。对于此问题，考虑使用$\epsilon$贪婪动作选择，样本平均动作值估计的赌博机算法，对于所有a，初始估计为$Q_1(a) = 0$。假设动作和奖励的初始序列是$A_1 = 1$，$R_1 = 1$，$A_2 = 2$，$R_2 = 1$，$A_3 = 2$，$R_3 = 2$，$A_4 = 2$，$R_4 = 2$，$A_5 = 3$，$R_5 = 0$。在某些时间步骤中，$\epsilon$情况可能已经发生，导致随机选择动作。在哪个时间步骤确实发生了？在哪些时间步骤可能发生？**
	
	在时间步骤2确实发生了。在时间步骤1、3、4、5可能发生。
3. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.2.png)**图** 10臂赌博机试验中的$\epsilon$贪婪行动价值方法的平均表现。 这些数据是2000轮以上不同赌博机问题的平均值。所有方法都使用样本平均值作为其行动价值估计。
	
	**在图所示的比较中，从累积奖励和选择最佳动作的概率来看，哪种方法在长期运行中表现最佳？会有多好？定量地表达你的答案。**
	
	$\epsilon = 0.01$
4. $$Q_{n + 1} = Q_n + \alpha(R_n - Q_n) = (1 - \alpha)^nQ_1 + \sum_{i = 1}^n\alpha(1 - \alpha)^{n - i}R_i$$
	**如果步长参数$\alpha_n$不是常数，则估计值$Q_n$是先前收到的奖励的加权平均值，其权重不同于给出的权重。对于一般情况，就步长参数的顺序而言，每个先前奖励的权重是什么？**
	
	$\alpha_i\prod_{j = i + 1}^n(1 - \alpha_j)R_i$
5. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.3.png)**图** 乐观的初始行动价值估计对10臂赌博机试验的影响。两种方法都使用恒定的步长参数，$\alpha = 0.1$。
	
	**_神秘的尖峰_ 图所示的结果应该非常可靠，因为它们是超过2000个随机选择的10臂赌博机任务的平均值。那么，为什么乐观方法曲线的早期会出现振荡和峰值？换句话说，什么可能使这种方法在特定的早期步骤中表现得更好或更差？**
6. **_无偏恒定步长技巧_ 在本章的大部分内容中，我们使用样本平均值来估计动作值，因为样本平均值不会产生恒定步长的初始偏差。然而，样本平均值并不是一个完全令人满意的解决方案，因为它们可能在非平稳问题上表现不佳。是否有可能避免不变步长的偏差，同时保留其对非平稳问题的优势？一种方法是使用步长$\beta_n = \alpha / \bar o_n$处理特定动作的第n个奖励，其中$\alpha > 0$是常规常量步长，$\bar o_n$是从0开始的跟踪：$\bar o_n = \bar o_{n - 1} + \alpha(1 - \bar o_{n - 1})\text{ for }n > 0\text{, with }\bar o_0 = 0$**
	
	**进行分析，以表明$𝑄_n$是指数的新近加权平均值，_没有初始偏差_。**
	
	$Q_{n + 1} = Q_n + \alpha(R_n - Q_n) \Rightarrow Q_{n + 1} = Q_n + \beta_n(R_n - Q_n) = Q_n + (\alpha / \bar o_n)(R_n - Q_n) = Q_n + \{\alpha / [\bar o_{n - 1} + \alpha(1 - \bar o_{n - 1})]\}(R_n - Q_n) = (1 - \alpha)^n\prod_{i = 1}^n\frac{\bar o_{i - 1}}{\bar o_{i - 1} + \alpha(1 - \bar o_{i - 1})}Q_1 + \alpha\sum_{i = 1}^n\frac{(1 - \alpha)^{n - i}}{\bar o_{i - 1} + \alpha(1 - \bar o_{i - 1})}\prod_{j = i + 1}^n\frac1{\bar o_{j - 1} + \alpha(1 - \bar o_{j - 1})}R_i$
7. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.4.png)**图** 10臂赌博机试验上UCB动作选择的平均表现。如图所示，除了在前$k$个步骤中，当它在尚未尝试的动作中随机选择时，UCB通常比$\epsilon$贪婪动作选择更好。
	
	**_USB尖峰_ 在图中，UCB算法在第11步显示出明显的性能峰值。为什么是这样？请注意，为了使您的答案完全令人满意，它必须解释为什么奖励在第11步增加以及为什么在随后的步骤中减少。提示：如果$c = 1$，则尖峰不太突出。**
8. **指出在两个动作的情况下，soft-max分布与统计学和人工神经网络中经常使用的逻辑或sigmoid函数给出的相同。**
# 马尔可夫决策过程
## 习题
1. **为什么在马尔可夫奖励过程中需要有折扣因子？**
	1. 首先，是有些马尔可夫过程是环状的，它并没有终点，所以我们想避免无穷的奖励。
	2. 另外，我们想把不确定性也表示出来，希望尽可能快地得到奖励，而不是在未来的某个时刻得到奖励。
	3. 接上一点，如果这个奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面才可以得到奖励。
	4. 还有，在有些时候，折扣因子也可以设为0。当它被设为0后，我们就只关注它当前的奖励。我们也可以把它设为1，设为1表示未来获得的奖励与当前获得的奖励是一样的。
	
	所以，折扣因子可以作为强化学习智能体的一个超参数进行调整，然后就会得到不同行为的智能体。
2. **为什么矩阵形式的贝尔曼方程的解析解比较难求得？**
	
	通过矩阵求逆的过程，我们就可以把$V$的解析解求出来。但是这个矩阵求逆的过程的复杂度是$O(N^3)$，所以当状态非常多的时候，比如从10个状态到1000个状态，到100万个状态，那么当我们有100万个状态的时候，转移矩阵就会是一个100万乘100万的矩阵。对于这样一个大矩阵进行求逆是非常困难的，所以这种通过解析解去解的方法，只能应用在很小量的马尔可夫奖励过程中。
3. **计算贝尔曼方程的常见方法有哪些，它们有什么区别？**
	1. 蒙特卡洛方法：可用来计算价值函数的值。以小船示例为例，当得到一个马尔可夫奖励过程后，我们可以从某一个状态开始，把小船放到水中，让它“随波逐流”，这样就会产生一条轨迹，从而得到一个折扣后的奖励$g$ 。当积累该奖励到一定数量后，直接除以轨迹数量，就会得到其价值函数的值。
	2. 动态规划方法：可用来计算价值函数的值。通过一直迭代对应的贝尔曼方程，最后使其收敛。当最后更新的状态与上一个状态区别不大的时候，通常是小于一个阈值$\gamma$时，更新就可以停止。
	3. 以上两者的结合方法：我们也可以使用时序差分学习方法，其为动态规划方法和蒙特卡洛方法的结合。
4. **马尔可夫奖励过程与马尔可夫决策过程的区别是什么？**
	
	相对于马尔可夫奖励过程，马尔可夫决策过程多了一个决策过程，其他的定义与马尔可夫奖励过程是类似的。由于多了一个决策，多了一个动作，因此状态转移也多了一个条件，即执行一个动作，导致未来状态的变化，其不仅依赖于当前的状态，也依赖于在当前状态下智能体采取的动作决定的状态变化。对于价值函数，它也多了一个条件，多了一个当前的动作，即当前状态以及采取的动作会决定当前可能得到的奖励的多少。
	
	另外，两者之间是有转换关系的。具体来说，已知一个马尔可夫决策过程以及一个策略$\pi$时，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程中，状态的转移函数$P(s' | s, a)$是基于它的当前状态和当前动作的，因为我们现在已知策略函数，即在每一个状态，我们知道其采取每一个动作的概率，所以我们就可以直接把这个动作进行加和，就可以得到对于马尔可夫奖励过程的一个转移概率。同样地，对于奖励，我们可以把动作去掉，这样就会得到一个类似于马尔可夫奖励过程的奖励。
5. **马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的结构或者计算方面的差异有哪些？**
	
	对于马尔可夫链，它的转移概率是直接决定的，即从当前时刻的状态通过转移概率得到下一时刻的状态值。但是对于马尔可夫决策过程，其中间多了一层动作的输出，即在当前这个状态，首先要决定采取某一种动作，再通过状态转移函数变化到另外一个状态。所以在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫过程的不同之处。在马尔可夫决策过程中，动作是由智能体决定的，所以多了一个组成部分，智能体会采取动作来决定未来的状态转移。
6. **我们如何寻找最佳策略，寻找最佳策略方法有哪些？**
	
	本质来说，当我们取得最佳价值函数后，我们可以通过对$Q$函数进行最大化，从而得到最佳价值。然后，我们直接对$Q$函数取一个让动作最大化的值，就可以直接得到其最佳策略。具体方法如下，
	1. 穷举法（一般不使用）：假设我们有有限个状态、有限个动作可能性，那么每个状态我们可以采取$A$种动作策略，那么总共就是$|A|^{|S|}$个可能的策略。我们可以把他们穷举一遍，然后算出每种策略的价值函数，对比一下就可以得到最佳策略。但是这种方法的效率极低。
	2. 策略迭代：一种迭代方法，其由两部分组成，以下两个步骤一直在迭代进行，最终收敛，其过程有些类似于机器学习中的EM算法（期望-最大化算法）。第一个步骤是策略评估，即当前我们在优化这个策略$\pi$，在优化过程中通过评估从而得到一个更新的策略；第二个步骤是策略提升，即取得价值函数后，进一步推算出它的$Q$函数，得到它的最大值。
	3. 价值迭代：我们一直迭代贝尔曼最优方程，通过迭代，其能逐渐趋向于最佳策略，这是价值迭代方法的核心。我们为了得到最佳的$V^*$，对于每个状态的$V^*$值，直接使用贝尔曼最优方程进行迭代，迭代多次之后它就会收敛到最佳策略及其对应的状态，这里是没有策略函数的。
## 面试题
1. **请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？**
	
	马尔可夫过程是一个二元组$<S, P>$，$S$为状态集合，$P$为状态转移函数；马尔可夫决策过程是一个五元组$<S, P, A, R, \gamma >$，其中$R$表示从$S$到$S'$能够获得的奖励期望，$\gamma$为折扣因子，$A$为动作集合；马尔可夫最重要的性质是下一个状态只与当前状态有关，与之前的状态无关，也就是 $p(s_{t + 1} | s_t) = p(s_{t + 1} | s_1, s_2, \dots, s_t)$。
2. 请问我们一般怎么求解马尔可夫决策过程？
	
	我们求解马尔可夫决策过程时，可以直接求解贝尔曼方程或动态规划方程：$V(s) = R(S) + \gamma\sum_{s' \in S}p(s' | s)V(s')$
	
	特别地，其矩阵形式为$V = R + \gamma PV$。但是贝尔曼方程很难求解且计算复杂度较高，所以可以使用动态规划、蒙特卡洛以及时序差分等方法求解。
3. **请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？**
	
	如果不具备马尔可夫性，即下一个状态与之前的状态也有关，若仅用当前的状态来求解决策过程，势必导致决策的泛化能力变差。为了解决这个问题，可以利用循环神经网络对历史信息建模，获得包含历史信息的状态表征，表征过程也可以使用注意力机制等手段，最后在表征状态空间求解马尔可夫决策过程问题。
4. **请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。**
	1. 基于状态价值函数的贝尔曼方程：$V_\pi(s) = \sum_a\pi(a|s)\sum_{s', r}p(s', r | s, a)(r(s, a) + \gamma V_\pi(s'))$
	2. 基于动作价值函数的贝尔曼方程：$Q_\pi(s, a) = \sum_{s', r}p(s', r|s, a)(r(s', a) + \gamma V_\pi(s'))$
5. **请问最佳价值函数$V_*$和最佳策略$π^*$为什么等价呢？**
	
	最佳价值函数的定义为$V^*(s) = \max_\pi V_\pi(s)$，即我们搜索一种策略$\pi$来让每个状态的价值最大。$V^*$就是到达每一个状态其的最大价值，同时我们得到的策略就可以说是最佳策略，即$π^*(s) = \argmax_\pi V_\pi(s)$。最佳策略使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个最佳价值函数，就可以说某一个马尔可夫决策过程的环境被解。在这种情况下，其最佳价值函数是一致的，即其达到的上限的值是一致的，但这里可能有多个最佳策略对应于相同的最佳价值。
6. **能不能手写一下第$n$步的价值函数更新公式呀？另外，当$n$越来越大时，价值函数的期望和方差是分别变大还是变小呢？**
	
	$n$越大，方差越大，期望偏差越小。价值函数的更新公式如下：$Q(S, A) \leftarrow Q(S, A) + \alpha(\sum_{i = 1}^n\gamma^{i - 1}r_{t + i} + \gamma^n\max_aQ(S', a) - Q(S, A))$
## 练习
1. **MDP框架是否足以有效地代 _所有_ 目标导向的学习任务？你能想到任何明显的例外吗？**
2. **考虑驾驶问题。你可以根据加速器，方向盘和制动器（即你的身体与机器接触的位置）来定义动作。或者你可以将它们定义得更远，比如橡胶与道路相遇，考虑你的动作是轮胎扭矩。或者你可以进一步定义它们，比如说，你的大脑掌控身体，肌肉抽搐的动作来控制你的四肢。或者你可以达到一个更高的层次，说你的行动是你选择开车的地方。什么是个体和环境之间合适的层次和位置分界？在什么基础上，该线的一个位置优先于另一个？是否有任何根本原因选择一个位置而不是另一个位置，还是随意选择？**
- **例：环保机器人**
	
	移动机器人的工作是在办公室环境中收集空的汽水罐。它有用于检测汽水罐的传感器，以及可以将它们拾起并放置在机箱中的臂和夹具；它使用可充电电池供电。机器人的控制系统具有用于解释传感器信息，用于导航以及用于控制手臂和夹具的部件。关于如何搜索汽水罐的高级决策是由强化学习个体根据电池的当前充电水平做出的。举一个简单的例子，我们假设只能区分两个电荷电平，包括一个小的状态集$\mathcal S = \{高, 低\}$。在每个状态，个体可以决定是否（1）在一段时间内主动 **搜索** 汽水罐，（2）保持静止并 **等待** 某人给它汽水罐，或（3）返回其本垒为电池 **充电**。当能量水平很**高**时，充电总是愚蠢的，所以我们不会将其包含在为此状态设定的动作中。 动作集是$\mathcal A(高) = \{搜索, 等待\}$和$\mathcal A(低) = \{搜索, 等待, 充电\}$。
	
	奖励在大多数情况下为零，但是当机器人固定空罐时变为正值，或者如果电池完全耗尽则变为负值。找到汽水罐的最好方法是主动搜索它们，但这会耗尽机器人的电池电量，而等待则不会。每当机器人正在搜索时，存在其电池耗尽的可能性。在这种情况下，机器人必须关闭并等待获救（产生低回报）。如果电池电量水平**高**，则可以始终完成一段主动搜索而没有耗尽电池的风险。以**高**电量水平开始的搜索周期使电量水平以概率$\alpha$保持并且以概率$1 − \alpha$降低至**低**电量水平。另一方面，当电量水平**低**时进行的搜索周期使其以概率$\beta$变**低**并且以概率$1 − \beta$消耗电池。在后一种情况下，必须拯救机器人，然后将电池重新充电至**高**电量水平。机器人收集的每个汽水罐都可以作为单位奖励计算，而每当机器人必须获救时，奖励为$-3$。用$r_{搜索}$和$r_{等待}$，其中$r_{搜索} > r_{等待}$，分别表示机器人在搜索和等待时将收集的预期罐数（以及预期的奖励）。最后，假设在跑步回家期间不能收集罐头，并且在电池耗尽的过程中不能收集罐头。这个系统是一个有限的MDP，我们可以记下转移概率和预期的奖励，动态如左表所示：![](https://rl.qiwihui.com/zh-cn/latest/_images/table_figure.png)请注意，表中有一行代表当前状态$s$，动作$a$，$a \in \mathcal A(s)$的每种可能组合。某些转换的概率为零，因此没有为它们指定预期的奖励。右侧所示是另一种有用的方法，可以总结有限MDP的动态，称为 _转换图_。 有两种节点：_状态节点_ 和 _动作节点_。每个可能的状态都有一个状态节点（由状态名称标记的大圆圈），以及每个状态-动作对的动作节点（由行动名称标记并由线连接的小实心圆圈）。从状态$s$开始并采取动$a$，你将沿着从状态节点$s$到动作节点$(s, a)$的线路移动。然后，环境通过离开动作节点$(s, a)$的箭头之一转换到下一个状态的节点。每个箭头对应一个三元组$(s, s', a)$，其中$s'$是下一个状态，我们用转移概率$p(s' | s, a)$标记箭头，以及该转换的预期回报$r(s, a, s')$。请注意，标记离开动作节点的箭头的转移概率和总是为$1$。
3. **给出一个类似于例中的表，但是对于$p(s', r | s, a)$。它应该有$s$，$a$，$s'$，$r$和$p(s', r | s, a)$的列，以及$p(s', r | s, a) > 0$的每个4元组的行。**
4. **$\sum_{s' \in \mathcal S}\sum_{r \in \mathcal R}p(s', r | s, a) = 1\text{，对所有}s \in \mathcal S\text{，}a \in \mathcal A(s)$等式是针对连续的情况，需要进行修改（非常轻微）以应用于情节任务。通过给出修改版本，表明你知道所需的修改。
	
	$\sum_{s' \in \mathcal S \cup S^+}\sum_{r \in \mathcal R}p(s', r | s, a) = 1\text{，对所有}s \in \mathcal S\text{，}a \in \mathcal A(s)$
5. **假设你将杆平衡作为一个情节性任务，但是也使用了衰减因子，除了$-1$是失败之外，所有奖励都是零。那么每次回报是多少？这个回报与有衰减的持续任务有什么不同？**
	
	$-\gamma^{K - 1}$
6. **想象一下，你正在设计一个运行迷宫的机器人。你决定在逃离迷宫时奖励$+1$，在其他时候奖励零。任务似乎自然地分解为情景，即连续贯穿迷宫的运行，所以你决定把它当作一个偶然的任务，其目标是最大化预期的总奖励。运行学习个体一段时间后，您会发现它从迷宫中逃脱没有任何改善。出了什么问题？你是否有意识地向个体传达了你希望它实现的目标？**
7. **假设$\gamma = 0.5$并且接收以下奖励序列$R_1 = 1$，$R_2 = 2$， $R_3 = 6$，$R_4 = 3$，并且$R_5 = 2$，其中$T = 5$。$G_0$，$G_1$，...，$G_5$是多少？提示：反向工作。**
	
	$\begin{split}G_t = r_{t + 1} + \gamma G_{t + 1} & \Rightarrow G_{4} = 2 + 0.5 \times 0 = 2 \\ & \Rightarrow G_{3} = 3 + 0.5 \times 2 = 4 \\ & \Rightarrow G_{2} = 6 + 0.5 \times 4 = 8 \\ & \Rightarrow G_{1} = 2 + 0.5 \times 8 = 6 \\ & \Rightarrow G_{0} = 1 + 0.5 \times 6 = 4\end{split}$
8.  **假设$\gamma = 0.9$并且奖励序列是$R_1 = 2$，接着是无限序列的7s。$G_1$和$G_0$是什么？**
	
	$G_1 = 70$
	
	$G_t = r_{t + 1} + \gamma G_{t + 1} \Rightarrow G_0 = 2 + 0.9 \times 70 = 65$
9. **证明$\sum_{k = 0}^\infty\gamma^k = \frac1{1 - \gamma}$。**
10. **如果当前状态为$S_t$，并且根据随机策略$\pi$选择动作，则对于$\pi$和四参数函数$p$，$R_{t + 1}$的期望是多少？**
11. **用$q_\pi$和$\pi$给出$v_\pi$的等式。**
	
	$V_\pi(s) = \sum_{a \in A}\pi(a | s)Q_\pi(s, a)$
12. 根据$v_\pi$和四参数$p$给出$q_\pi$的等式。
- **示例3.5：网格世界** 图（左图）显示了简单有限MDP的矩形网格世界表示。网格的单元格对应于环境的状态。在每个单元格中，可以有四个动作：北，南，东，西，这明确让个体在网格上的相应方向上移动一个单元格。使个体离开网格的操作会使其位置保持不变，但会导致-1的奖励。除了将个体从特殊状态$A$和$B$移出的行为，其他行为奖励值为$0$。在状态$A$，所有四个动作都会产生$+10$的奖励，并将个体送到$A'$ 。从状态$B$，所有动作都会获得$+5$的奖励，并将个体转到$B'$。![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-3.2.png)**图** 网格世界的例子：等概率随机随机策略的特殊奖励动态（左）和状态值函数（右）。
13. **图（右）所示的值函数$v_\pi$，贝尔曼方程必须保持函数中的每个状态。以数字方式显示该等式的相对于其四个相邻状态，价值为$+2.3$，$+0.4$，$−0.4$和$+0.7$，保持中心状态，值为$+0.7$。（这些数字只能精确到一位小数。）**
	
	$V_\pi(s) = \sum_{a \in A}\pi(a | s)(R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)V_\pi(s')) \Rightarrow V_\pi(s) = \frac14 \times (0 + 1 \times 1 \times 2.3) + \frac14 \times (0 + 1 \times 1 \times 0.4) + \frac14 \times [0 + 1 \times 1 \times (-0.4)] + \frac14 \times (0 + 1 \times 1 \times 0.7) = 0.7$
14. **在网格世界的例子中，奖励对于目标是正的奖励，对于走进世界的边缘是负的惩罚，而在其余的时间里是零。这些奖励的符号是重要的吗，还是只是他们之间的间隔？请证明，使用$G_t = \sum_{k = 0}^\infty R_{t + k + 1}$， 向所有奖励添加常量$c$会使所有状态的价值增加一个常数$v_c$ ，因此不会影响任何策略下任何状态的相对值。$v_c$关于$c$和$\gamma$是什么？**
	
	$G_t = \sum_{k = 0}^\infty R_{t + k + 1} \Rightarrow G_t' = \sum_{k = 0}^\infty cR_{t + k + 1} = c\sum_{k = 0}^\infty R_{t + k + 1} = cG_t$
15. **现在考虑在情节任务，例如走迷宫中给所有的奖励增加一个常量$c$。这是否会有什么影响，还是会像以上那些连续任务那样保持不变？是或者否，为什么？举个例子。**
	
	是。$G_t = r_{t + 1} + \gamma r_{t + 2} + \dots + \gamma^{T - t - 1}r_T \Rightarrow G_t' = cr_{t + 1} + \gamma cr_{t + 2} + \dots + \gamma^{T - t - 1}cr_T = c(r_{t + 1} + \gamma r_{t + 2} + \dots + \gamma^{T - t - 1}r_T) = cG_t$
16. **动作价值，即$q_\pi$的贝尔曼方程是什么？必须根据状态—动作对$(s, a)$的可能后继的动作价值$q_\pi(s', a')$给出动作值$q_\pi(s, a)$。提示：对应于该方程的备用图在右图中给出。**![](https://rl.qiwihui.com/zh-cn/latest/_images/q_pi_backup_diagram.png)
	 
	$q\pi$的备份图
	
	$Q_\pi(s, a) = R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)\sum_{a' \in A}\pi(a', s')Q_\pi(s', a')$
17. **状态的值取决于在该状态下可能的动作的值以及当前策略下每个动作的可能性。我们可以根据状态的小的备份图来考虑这一点，并考虑到每个可能的操作：![](https://rl.qiwihui.com/zh-cn/latest/_images/exercise-3.18.png)根据给定$S_t = s$的预期叶节点$q_\pi(s, a)$的值，给出对根节点$v_\pi(s)$的值的对应于该直觉和图表的方程。这个方程式应该包含一个符合策略$\pi$条件的预期。然后给出第二个等式，其中期望值以$\pi(a | s)$方式明确地写出，使得等式中不出现预期值符号。**
	
	$V_\pi(s) = \sum_{a \in A}\pi(a | s)Q_\pi(s, a)$
18. **动作值$q_\pi(s, a)$取决于预期的下一个奖励和剩余奖励的预期总和。再次，我们可以通过一个小的备份图来考虑这一点，这一个根源于一个动作（状态—动作对），并分支到可能的下一个状态：![](https://rl.qiwihui.com/zh-cn/latest/_images/exercise-3.19.png)给定$S_t = s$和$A_t = a$，根据预期的下一个奖励$R_{t + 1}$和预期的下一个状态值$v_\pi(S_{t + 1})$，给出与这个直觉和图表对应的方程式的动作值$q_\pi(s, a)$。这个方程式应该包括期望值，但 _不包括_ 一个符合策略的条件。**
	
	$Q_\pi(s, a) = R(s, a) + \gamma\sum_{s' \in S}p(s' | s, a)V_\pi(s')$
19. **考虑右侧显示的持续MDP。唯一的决定是在顶点状态，有左 右两个动作可选。数字显示每次行动后收到的确定奖励。有两个确定性的策略，$\pi_左$和$\pi_右$。如果$\gamma = 0$，哪一种策略是最优的？如果$\gamma = 0.9$？如果$\gamma = 0.5$呢？**![](https://rl.qiwihui.com/zh-cn/latest/_images/exercise-3.22.png)
