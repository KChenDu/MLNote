# 绪论
## 习题
1. **强化学习的基本结构是什么？**
	
	本质上是智能体与环境的交互。具体地，当智能体在环境中得到当前时刻的状态后，其会基于此状态输出一个动作，这个动作会在环境中被执行并输出下一个状态和当前的这个动作得到的奖励。智能体在环境里存在的目标是最大化期望累积奖励。
2. **强化学习相对于监督学习为什么训练过程会更加困难？**
	1. 强化学习处理的大多是序列数据，其很难像监督学习的样本一样满足独立同分布条件。
	2. 强化学习有奖励的延迟，即智能体的动作作用在环境中时，环境对于智能体状态的奖励存在延迟，使得反馈不实时。
	3. 监督学习有正确的标签，模型可以通过标签修正自己的预测来更新模型，而强化学习相当于一个“试错”的过程，其完全根据环境的“反馈”更新对自己最有利的动作。
3. **强化学习的基本特征有哪些？**
	1. 有试错探索过程，即需要通过探索环境来获取对当前环境的理解。
	2. 强化学习中的智能体会从环境中获得延迟奖励。
	3. 强化学习的训练过程中时间非常重要，因为数据都是时间关联的，而不是像监督学习中的数据大部分是满足独立同分布的。
	4. 强化学习中智能体的动作会影响它从环境中得到的反馈。
4. **近几年强化学习发展迅速的原因有哪些？**
	1. 算力的提升使我们可以更快地通过试错等方法来使得智能体在环境里面获得更多的信息，从而取得更大的奖励。
	2. 我们有了深度强化学习这样一个端到端的训练方法，可以把特征提取、价值估计以及决策部分一起优化，这样就可以得到一个更强的决策网络。
5. **状态和观测有什么关系？**
	
	状态是对环境的完整描述，不会隐藏环境信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用同一个实值向量、矩阵或者更高阶的张量来表示状态和观测。
6. **一个强化学习智能体由什么组成？**
	1. 策略函数，智能体会用策略函数来选取它下一步的动作，策略包括随机性策略和确定性策略。
	2. 价值函数，我们用价值函数来对当前状态进行评估，即进入现在的状态可以对后面的奖励带来多大的影响。价值函数的值越大，说明进入该状态越有利。
	3. 模型，其表示智能体对当前环境状态的理解，它决定系统是如何运行的。
7. **根据强化学习智能体的不同，我们可以将其分为哪几类？**
	1. 基于价值的智能体。显式学习的是价值函数，隐式地学习智能体的策略。因为这个策略是从学到的价值函数里面推算出来的
	2. 基于策略的智能体。其直接学习策略，即直接给智能体一个状态，它就会输出对应动作的概率。当然在基于策略的智能体里面并没有去学习智能体的价值函数。
	3. 另外还有一种智能体，它把以上两者结合。把基于价值和基于策略的智能体结合起来就有了演员-评论员智能体。这一类智能体通过学习策略函数和价值函数以及两者的交互得到更佳的状态。
8. **基于策略迭代和基于价值迭代的强化学习方法有什么区别？**
	1. 基于策略迭代的强化学习方法，智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据该策略进行操作。强化学习算法直接对策略进行优化，使得制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。
	2. 基于价值迭代的方法只能应用在离散的环境下，例如围棋或某些游戏领域，对于行为集合规模庞大或是动作连续的场景，如机器人控制领域，其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。
	3. 基于价值迭代的强化学习算法有Q-learning、Sarsa等，基于策略迭代的强化学习算法有策略梯度算法等。
	4. 此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，从而取得更好的效果。
9. **有模型学习和免模型学习有什么区别？**
	
	针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。总体来说，有模型学习相比免模型学习仅仅多出一个步骤，即对真实环境进行建模。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。
10. **如何通俗理解强化学习？**
	
	环境和奖励函数不是我们可以控制的，两者是在开始学习之前就已经事先确定的。我们唯一能做的事情是调整策略，使得智能体可以在环境中得到最大的奖励。另外，策略决定了智能体的行为，策略就是给一个外界的输入，然后它会输出现在应该要执行的动作。
## 面试题
1. **看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？**
	
	强化学习包含环境、动作和奖励3部分，其本质是智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大，或者说是期望最大。
2. **请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？**
	
	首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建和训练。其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。另外，强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独立同分布条件。
3. **根据你的理解，你认为强化学习的使用场景有哪些呢？**
	
	7个字总结就是“多序列决策问题”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达的条件，即满足可学习条件。
4. **请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？**
	
	深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大。
5. **你了解有模型和免模型吗？两者具体有什么区别呢？**
	
	我认为两者的区别主要在于是否需要对真实的环境进行建模，免模型方法不需要对环境进行建模，直接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如果建模的环境与真实环境的差异较大，那么会限制其泛化性能。现在通常使用有模型方法进行模型的构建工作。
# 多臂赌博机问题
1. **在$\epsilon$贪婪动作选择中，对于两个动作的情况和$\epsilon = 0.5$，选择贪婪动作的概率是多少？**
	
	$1 - \epsilon + \epsilon\times\frac12 = 1 - 0.5 + 0.5\times\frac12 = \frac34$
2. **赌博机示例 考虑具有$k = 4$动作的$k$臂赌博机问题，表示为1，2，3和4。对于此问题，考虑使用$\epsilon$贪婪动作选择，样本平均动作值估计的赌博机算法，对于所有a，初始估计为$Q_1(a) = 0$。假设动作和奖励的初始序列是$A_1 = 1$，$R_1 = 1$，$A_2 = 2$，$R_2 = 1$，$A_3 = 2$，$R_3 = 2$，$A_4 = 2$，$R_4 = 2$，$A_5 = 3$，$R_5 = 0$。在某些时间步骤中，$\epsilon$情况可能已经发生，导致随机选择动作。在哪个时间步骤确实发生了？在哪些时间步骤可能发生？**
	
	在时间步骤2确实发生了。在时间步骤1、3、4、5可能发生。
3. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.2.png)**图** 10臂赌博机试验中的$\epsilon$贪婪行动价值方法的平均表现。 这些数据是2000轮以上不同赌博机问题的平均值。所有方法都使用样本平均值作为其行动价值估计。
	
	**在图所示的比较中，从累积奖励和选择最佳动作的概率来看，哪种方法在长期运行中表现最佳？会有多好？定量地表达你的答案。**
	
	$\epsilon = 0.01$
4. $$Q_{n + 1} = Q_n + \alpha(R_n - Q_n) = (1 - \alpha)^nQ_1 + \sum_{i = 1}^n\alpha(1 - \alpha)^{n - i}R_i$$
	**如果步长参数$\alpha_n$不是常数，则估计值$Q_n$是先前收到的奖励的加权平均值，其权重不同于给出的权重。对于一般情况，就步长参数的顺序而言，每个先前奖励的权重是什么？**
	
	$\alpha_i\prod_{j = i + 1}^n(1 - \alpha_j)R_i$
1. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.3.png)**图** 乐观的初始行动价值估计对10臂赌博机试验的影响。两种方法都使用恒定的步长参数，$\alpha = 0.1$。
	
	**_神秘的尖峰_ 图所示的结果应该非常可靠，因为它们是超过2000个随机选择的10臂赌博机任务的平均值。那么，为什么乐观方法曲线的早期会出现振荡和峰值？换句话说，什么可能使这种方法在特定的早期步骤中表现得更好或更差？**
6. **_无偏恒定步长技巧_ 在本章的大部分内容中，我们使用样本平均值来估计动作值，因为样本平均值不会产生恒定步长的初始偏差。然而，样本平均值并不是一个完全令人满意的解决方案，因为它们可能在非平稳问题上表现不佳。是否有可能避免不变步长的偏差，同时保留其对非平稳问题的优势？一种方法是使用步长$\beta_n = \alpha / \bar o_n$处理特定动作的第n个奖励，其中$\alpha > 0$是常规常量步长，$\bar o_n$是从0开始的跟踪：$\bar o_n = \bar o_{n - 1} + \alpha(1 - \bar o_{n - 1})\text{ for }n > 0\text{, with }\bar o_0 = 0$**
	
	**进行分析，以表明$𝑄_n$是指数的新近加权平均值，_没有初始偏差_。**
	
	$Q_{n + 1} = Q_n + \alpha(R_n - Q_n) \Rightarrow Q_{n + 1} = Q_n + \beta_n(R_n - Q_n) = Q_n + (\alpha / \bar o_n)(R_n - Q_n) = Q_n + \{\alpha / [\bar o_{n - 1} + \alpha(1 - \bar o_{n - 1})]\}(R_n - Q_n) = (1 - \alpha)^n\prod_{i = 1}^n\frac{\bar o_{i - 1}}{\bar o_{i - 1} + \alpha(1 - \bar o_{i - 1})}Q_1 + \alpha\sum_{i = 1}^n\frac{(1 - \alpha)^{n - i}}{\bar o_{i - 1} + \alpha(1 - \bar o_{i - 1})}\prod_{j = i + 1}^n\frac1{\bar o_{j - 1} + \alpha(1 - \bar o_{j - 1})}R_i$
7. ![](https://rl.qiwihui.com/zh-cn/latest/_images/figure-2.4.png)**图** 10臂赌博机试验上UCB动作选择的平均表现。如图所示，除了在前$k$个步骤中，当它在尚未尝试的动作中随机选择时，UCB通常比$\epsilon$贪婪动作选择更好。
	
	**USB尖峰_ 在图中，UCB算法在第11步显示出明显的性能峰值。为什么是这样？请注意，为了使您的答案完全令人满意，它必须解释为什么奖励在第11步增加以及为什么在随后的步骤中减少。提示：如果$c = 1$，则尖峰不太突出。**