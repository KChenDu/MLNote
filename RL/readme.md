1. 任务与奖赏
    
    强化学习任务通常用马尔可夫决策过程来描述：机器处于环境$E$中，状态空间为$X$，其中每个状态$x \in X$是机器感知到的环境的描述。若某个动作$a \in A$作用在当前状态$x$上，则潜在的转移函数$P$将使得环境从当前状态按某种概率转移到另一个状态；在转移到另一个状态的同时，环境会根据潜在的“奖赏”函数$R$反馈给机器一个奖赏。综合起来，强化学习任务对应了四元组$E = \langle X, A, P, R\rangle$，其中$P: X \times A \times X \rightarrow \mathbb R$指定了状态转移概率，$R: X \times A \times X \rightarrow \mathbb R$指定了奖赏；在有的应用中，奖赏函数可能仅与状态转移有关，即$R: X \times X \rightarrow \mathbb R$。

    机器要做的是通过在环境中不断地尝试而学得一个“策略”$\pi$，根据这个策略，在状态$x$下就能得知要执行的动作$a = \pi(x)$。策略有两种表示方法：一种是将策略表示为函数$\pi: X \rightarrow A$， 确定性策略常用这种表示；另一种是概率表示$\pi: X \times A \rightarrow \mathbb R$，随机性策略常用这种表示$\pi(x, a)$为状态$x$下选择动作$a$的概率，这里必须有$\sum_a\pi(x, a) = 1$。

    策略的优劣取决于长期执行这一策略后得到的累积奖赏。长期累积奖赏有多种计算方式，常用的有“T步累积奖赏“$\mathbb E[\frac1T\sum_{t = 1}^Tr_t]$和”$\gamma$折扣累积奖赏“$\mathbb E[\sum_{t = 0}^{+\infty}\gamma^tr_{t + 1}]$，其中$r_t$表示第$t$步获得的奖赏值，$\mathbb E$表示对所有随机变量求期望。
3. 有模型学习

    考虑多步强化学习任务，暂且先假定任务对应的马尔可夫决策过程四元组$E = \langle X, A, P, R\rangle$均为己知，这样的情形称为”模型己知“，即机器已对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。在己知模型的环境中学习称为”有模型学习“。此时，对于任意状态$x$, $x'$和动作$a$，在$x$状态下执行动作$a$转移到$x'$状态的概率$P_{x \rightarrow x'}^a$是己知的，该转移所带来的奖赏$R_{x \rightarrow x'}^a$也是已知的。

[返回](../readme.md)