1. 任务与奖赏
    
    强化学习任务通常用马尔可夫决策过程来描述：机器处于环境$E$中，状态空间为$X$，其中每个状态$x \in X$是机器感知到的环境的描述。若某个动作$a \in A$作用在当前状态$x$上，则潜在的转移函数$P$将使得环境从当前状态按某种概率转移到另一个状态；在转移到另一个状态的同时，环境会根据潜在的“奖赏”函数$R$反馈给机器一个奖赏。综合起来，强化学习任务对应了四元组$E = \langle X, A, P, R\rangle$，其中$P: X \times A \times X \rightarrow \mathbb R$指定了状态转移概率，$R: X \times A \times X \rightarrow \mathbb R$指定了奖赏；在有的应用中，奖赏函数可能仅与状态转移有关，即$R: X \times X \rightarrow \mathbb R$。

    机器要做的是通过在环境中不断地尝试而学得一个“策略”$\pi$，根据这个策略，在状态$x$下就能得知要执行的动作$a = \pi(x)$。策略有两种表示方法：一种是将策略表示为函数$\pi: X \rightarrow A$， 确定性策略常用这种表示；另一种是概率表示$\pi: X \times A \rightarrow \mathbb R$，随机性策略常用这种表示$\pi(x, a)$为状态$x$下选择动作$a$的概率，这里必须有$\sum_a\pi(x, a) = 1$。

    策略的优劣取决于长期执行这一策略后得到的累积奖赏。长期累积奖赏有多种计算方式，常用的有“T步累积奖赏“$\mathbb E[\frac1T\sum_{t = 1}^Tr_t]$和”$\gamma$折扣累积奖赏“$\mathbb E[\sum_{t = 0}^{+\infty}\gamma^tr_{t + 1}]$，其中$r_t$表示第$t$步获得的奖赏值，$\mathbb E$表示对所有随机变量求期望。
2. K-摇臂赌博机
    1. 探索与利用

        这里我们不妨先考虑比较简单的情形：最大化单步奖赏，即仅考虑一步操作。需注意的是，即使在这样的简化情形下，强化学习仍与监督学习有显著不同，因为机器需通过尝试来发现各个动作产生的结果，而没有训练数据告诉机器应当做哪个动作。

        实际上，单步强化学习任务对应了一个理论模型，即“K-摇臂赌博机”。K-摇臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。

        若仅为获知每个摇臂的期望奖赏，则可采用“仅探索”法：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币橄率作为其奖赏期望的近似估计。若仅为执行奖赏最大的动作，则可采用“仅利用”法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。显然，“仅探索”法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；“仅利用”法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

        事实上，“探索”（即估计摇臂的优劣）和“利用”（即选择当前最优摇臂）这两者是矛盾的，因为尝试次数（即总投币数）有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的“探索-利用窘境”。显然，欲累积奖赏最大，则必须在探索与利用之间达成较好的折中。
    2. $\epsilon$-贪心
    
        $\epsilon$-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以$\epsilon$的概率进行探索，即以均匀概率随机选取一个摇臂；以$1 - \epsilon$的概率进行利用，即选择当前平均奖赏最高的摇臂（若有多个，则随机选取一个）。

        若摇臂$k$被尝试了$n$次，得到的奖赏为$v_1, v_2, \dots, v_n$，则平均奖赏为$Q(k) = \frac1n\sum_{i = 1}^nv_i$

        显然，更高效的做法是对均值进行增量式计算，即每尝试一次就立即更新$Q(k)$。不妨用下标来表示尝试的次数，初始时$Q_0(k) = O$。对于任意的$n \ge 1$，若第$n - 1$次尝试后的平均奖赏为$Q_{n - 1}(k)$，则在经过第$n$次尝试获得奖赏$v_n$后，平均奖赏应更新为$Q_n(k) = \frac1n((n - 1)Q_{n - 1}(k) + v_n) = Q_{n - 1}(k) + \frac1n(v_n - Q_{n - 1}(k))$
        ![epsilon-贪心算法](epsilon-greedy.png "epsilon-贪心算法")
    3. Softmax

        Softmax算法中摇臂概率的分配是基于Boltzmann分布$P(k) = \frac{e^\frac{Q(k)}\tau}{\sum_{i = 1}^Ke^\frac{Q(i)}\tau}$
        
        其中，$Q(i)$记录当前摇臂的平均奖赏；$\tau > 0$称为“温度”，$\tau$越小则平均奖赏高的摇臂被选取的概率越高。$\tau$趋于0时Softmax将趋于“仅利用”，$\tau$趋于无穷大时Softmax则将趋于“仅探索”。
        ![Softmax算法](softmax.png "Softmax算法")
3. 有模型学习

    考虑多步强化学习任务，暂且先假定任务对应的马尔可夫决策过程四元组$E = \langle X, A, P, R\rangle$均为己知，这样的情形称为”模型己知“，即机器已对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。在己知模型的环境中学习称为”有模型学习“。此时，对于任意状态$x$, $x'$和动作$a$，在$x$状态下执行动作$a$转移到$x'$状态的概率$P_{x \rightarrow x'}^a$是己知的，该转移所带来的奖赏$R_{x \rightarrow x'}^a$也是已知的。

[返回](../readme.md)