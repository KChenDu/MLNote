1. 展开计算图
    $$\mathbf h^{(t)} = f(\mathbf h^{(t - 1)}, \mathbf x; \mathbf\theta) = g^{(t)}(\mathbf x^{(t)}, \mathbf x^{(t - 1)}, \mathbf x^{(t - 2)}, \dots, \mathbf x^{(2)}, \mathbf x^{(1)})$$
    函数$g(t)$将全部的过去序列$(\mathbf x^{(t)}, \mathbf x^{(t - 1)}, \mathbf x^{(t - 2)}, \dots, \mathbf x^{(2)}, \mathbf x^{(1)})$作为输入来生成当前状态，但是展开的循环架构允许我们将$g^{(t)}$分解为函数$f$的重复应用。因此，展开过程引入两个主要优点：
    1. 无论序列的长度，学成的模型始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作
    2. 我们可以在每个时间步使用相同参数的相同转移函数$f$
2. 循环神经网络

    循环神经网络中一些重要的设计模式包括以下几种：
    1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络
    2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络
    3. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络
    我们假设使用双曲正切激活函数。我们假定输出是离散的，如用于预测词或字符的RNN。表示离散变量的常规方式是把输出$o$作为每个离散变量可能值的非标准化对数概率。然后，我们可以应用softmax函数后续处理后，获得标准化后概率的输出向量$\hat{\mathbf y}$。RNN从特定的初始状态$h^{(0)}$开始前向传播。从$t = 1$到$t = \tau$的每个时间步，我们应用以下更新方程：
        $$\mathbf a^{(t)} = \mathbf b + W\mathbf h^{(t - 1)} + U\mathbf x^(t)$$
        $$\mathbf h^{(t)} = \tanh\mathbf a^{(t)}$$
        $$\mathbf o^{(t)} = \mathbf c + V\mathbf h^{(t)}$$
        $$\hat{\mathbf y}^{(t)} = \mathrm{softmax}(\mathbf o^{(t)})$$
        其中的参数的偏置向量$\mathbf b$和$\mathbf c$连同权重矩阵$U$、$V$和$W$，分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的连接。这个循环网络将一个输入序列映射到相同长度的输出序列。与$\mathbf x$序列配对的$\mathbf y$的总损失就是所有时间步的损失之和。例如，$L^{(t)}$为给定的$\mathbf x^{(1)}, \dots, \mathbf x^{(t)}$后$\mathbf y^{(t)}$的负对数似然，则$L(\{\mathbf x^{(1)}, \dots, \mathbf x^{(\tau)}\}， \{\mathbf y^{(1)}, \dots, \mathbf y^{(\tau)}\}) = \sum_t L^{(t)} = -\sum_t\log p_{\mathrm{model}}(y^{(t)} | \{\mathbf x^{(1)}, \dots, \mathbf x^{(t)}\})$

    4. 基于上下文的RNN序列建模

        另一种选择是只使用单个向量$\mathbf x$作为输入。当$\mathbf x$是一个固定大小的向量时，我们可以简单地将其看作产生$\mathbf y$序列RNN的额外输入。将额外输入提供到RNN的一些常见方法是：
        1. 在每个时刻作为一个额外输入
        2. 作为初始状态$\mathbf h^{(0)}$
        3. 结合两种方式

        RNN可以接收向量序列$\mathbf x^{(t)}$作为输入，而不是仅接收单个向量$\mathbf x$作为输入。RNN对应条件分布$P(\mathbf y^{(1)}, \dots, \mathbf y^{(\tau)} | \mathbf x^{(1)}, \dots, \mathbf x^{(\tau)})$，并在条件独立的假设下这个分布分解为$\prod_tP(\mathbf y^{(t)} | \mathbf x^{(1)}, \dots, \mathbf x^{(\tau)})$
3. 双向RNN

    顾名思义，双向RNN结合时间上从序列起点开始移动的RNN和另一个时间上从序列末尾开始移动的RNN。这个想法可以自然地扩展到2维输入，如图像，由四个RNN组成，每一个沿着四个方向中的一个计算：上、下、左、右。如果RNN能够学习到承载长期信息，那在2维网格每个点$(i, j)$的输出$O_{i, j}$就能计算一个能捕捉到大多局部信息但仍依赖于长期输入的表示。相比卷积网络，应用于图像的RNN计算成本通常更高，但允许同一特征图的特征之间存在长期横向的相互作用。实际上，对于这样的RNN，前向传播公式可以写成表示使用卷积的形式，计算自底向上到每一层的输入（在整合横向相互作用的特征图的循环传播之前）。
4. 基于编码-解码的序列到序列架构

    前一系统是对另一个机器翻译系统产生的建议进行评分，而后者使用独立的循环网络生成翻译。这个想法非常简单：（1）编码器RNN处理输入序列。编码器输出上下文C（通常是最终隐藏状态的简单函数）。（2）解码器则以固定长度的向量为条件产生输出序列$Y = \mathbf y^{(1)}, \dots, \mathbf y^{(n_y)}$。

    此架构的一个明显不足是，编码器RNN输出的上下文C的维度太小而难以适当地概括一个长序列。
5. 长期依赖的挑战
    
    根本问题是，经过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但对优化过程影响很大）。

    特别地，循环神经网络所使用的函数组合有点像矩阵乘法。我们可以认为，循环联系$\mathbf h^{(t)} = W^\top\mathbf h^{(t - 1)} = (W^t)^\top\mathbf h^{(0)}$

    而当$W$符合下列形式的特征分解
        $$W = Q\Lambda Q^\top$$
    其中$Q$正交，循环性可进一步简化为$\mathbf h^{(t)} = Q\Lambda^tQ\mathbf h^{(0)}$
    
    特征值提升到$t$次后，导致幅值不到一的特征值衰减到零，而幅值大于一的就会激增。任何不与最大特征向量对齐的$h(0)$的部分将最终被丢弃。
10. 长短期记忆和其他门控RNN

    1. LSTM

        LSTM循环网络除了外部的RNN循环外，还具有内部的“LSTM细胞“循环（自环），因此LSTM不是简单地向输入和循环单元的仿射变换之后施加一个逐元素的非线性。

        自环的权重（或相关联的时间常数）由**遗忘门**$f_i^{(t)}$控制（时刻$t$和细胞$i$），由sigmoid单元将权重设置为0和1之间的值：$f_i^{(t)} = \sigma(b_i^f + \sum_jU_{i, j}^fx_j^{(t)} + \sum_jW_{i, j}^fh_j^{(t - 1)})$，其中$\mathbf x^{(t)}$是当前输入向量，$\mathbf h^t$是当前隐藏层向量，$\mathbf h^t$包含所有LSTM细胞的输出。$\mathbf b^f$，$U^f$，$W^f$分别是偏置、输入权重和遗忘门的循环权重。因此 LSTM 细胞内部状态以如下方式更新，其中有一个条件的自环权重$f_i^{(t)}$：$s_i^{(t)} = f_i^{(t)}s_i^{(t - 1)} + g_i^{(t)}\sigma(b_i^f + \sum_jU_{i, j}x_j^{(t)} + \sum_jW_{i, j}h_j^{(t - 1)})$其中$\mathbf b$，$U$，$W$分别是LSTM细胞中的偏置、输入权重和遗忘门的循环权重。
        
        **外部输入门**单元$g_i^{(t)}$以类似遗忘门（使用sigmoid获得一个0和1之间的值）的方式更新，但有自身的参数：$g_i^{(t)} = \sigma(b_i^g + \sum_jU_{i, j}^gx_j^{(t)} + \sum_jW_{i, j}^gh_j^{(t - 1)})$

        LSTM细胞的输出$h_i^{(t)}$也可以由**输出门**$q_i^{(t)}$关闭（使用sigmoid单元作为门控）：
            $$h_i^{(t)} = \tanh(s_i^{(t)})q_i^{(t)}$$
            $$q_i^{(t)} = \sigma(b_i^o + \sum_jU_{i, j}^ox_j^{(t)} + \sum_jW_{i, j}^oh_j^{(t - 1)})$$
        其中$\mathbf b^o$，$U^o$，$W^o$分别是偏置、输入权重和遗忘门的循环权重。

        LSTM网络比简单的循环架构更易于学习长期依赖，先是用于测试长期依赖学习能力的人工数据集，然后是在具有挑战性的序列处理任务上获得最先进的表现。
    2. 其他门控RNN

        与LSTM的主要区别是，单个门控单元同时控制遗忘因子和更新状态单元的决定。更新公式如下：
            $$h_i^{(t)} = u_i^{(t - 1)}h_i^{(t - 1)} + (1 - u_i^{(t - 1)})\sigma(b_i + \sum_jU_{i, j}x_j^{(t)} + \sum W_{i, j}r_j^{(t - 1)}h_j^{(t - 1)})$$
        其中$\mathbf u$代表”更新“门，$\mathbf r$表示”复位“门。它们的值就如通常所定义的：$u_i^{(t)} = \sigma(b_i^u + \sum_jU_{i, j}^ux_j^{(t)} + \sum_jW_{i, j}^uh_j^{(t)})$和$r_i^{(t)} = \sigma(b_i^r + \sum_jU_{i, j}^rx_j^{(t)} + \sum_jW_{i, j}^rh_j^{(t)})$。复位和更新门能独立地”忽略“状态向量的一部分。更新门像条件渗漏累积器一样可以线性门控任意维度，从而选择将它复制（在sigmoid的一个极端）或完全由新的”目标状态“值（朝向渗漏累积器的收敛方向）替换并完全忽略它（在另一个极端）。复位门控制当前状态中哪些部分用于计算下一个目标状态，在过去状态和未来状态之间引入了附加的非线性效应。
11. 优化长期依赖
    1. 截断梯度

        一种选择是在参数更新之前，逐元素地截断小批量产生的参数梯度。另一种是在参数更新之前截断梯度$\mathbf g$的范数$\|\mathbf g\|$：if $\|\mathbf g\| > v$ $g \leftarrow \frac{\mathbf gv}{\|\mathbf g\|}$，其中v是范数上界，$\mathbf g$用来更新参数。因为所有参数（包括不同的参数组，如权重和偏置）的梯度被单个缩放因子联合重整化，所以后一方法具有的优点是保证了每个步骤仍然是在梯度方向上的，但实验表明两种形式类似。虽然参数更新与真实梯度具有相同的方向梯度，经过梯度范数截断，参数更新的向量范数现在变得有界。这种有界梯度能避免执行梯度爆炸时的有害一步。

[返回](readme.md)