{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3670"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Invalid split train[:10%]. Available splits are: ['train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_set_raw, valid_set_raw, train_set_raw \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf_flowers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[:10\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[10\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m:25\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[25\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m:]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# extra code â€“ displays the first 9 images in the validation set\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py:52\u001b[0m, in \u001b[0;36mdisallow_positional_args.<locals>.disallow_positional_args_dec\u001b[1;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m _check_no_positional(fn, args, ismethod, allowed\u001b[38;5;241m=\u001b[39mallowed)\n\u001b[0;32m     51\u001b[0m _check_required(fn, kwargs)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py:312\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    309\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_memory)\n\u001b[0;32m    310\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffle_files\u001b[39m\u001b[38;5;124m\"\u001b[39m, shuffle_files)\n\u001b[1;32m--> 312\u001b[0m ds \u001b[38;5;241m=\u001b[39m dbuilder\u001b[38;5;241m.\u001b[39mas_dataset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mas_dataset_kwargs)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_info:\n\u001b[0;32m    314\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ds, dbuilder\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py:52\u001b[0m, in \u001b[0;36mdisallow_positional_args.<locals>.disallow_positional_args_dec\u001b[1;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m _check_no_positional(fn, args, ismethod, allowed\u001b[38;5;241m=\u001b[39mallowed)\n\u001b[0;32m     51\u001b[0m _check_required(fn, kwargs)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:420\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[1;34m(self, split, batch_size, shuffle_files, decoders, as_supervised, in_memory)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m    412\u001b[0m build_single_dataset \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_single_dataset,\n\u001b[0;32m    414\u001b[0m     shuffle_files\u001b[38;5;241m=\u001b[39mshuffle_files,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m     in_memory\u001b[38;5;241m=\u001b[39min_memory,\n\u001b[0;32m    419\u001b[0m )\n\u001b[1;32m--> 420\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_single_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:136\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[0;32m    134\u001b[0m   types\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mtuple\u001b[39m(types)):\n\u001b[1;32m--> 136\u001b[0m   mapped \u001b[38;5;241m=\u001b[39m [map_nested(function, v, dict_only, map_tuple)\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct]\n\u001b[0;32m    138\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:136\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    134\u001b[0m   types\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mtuple\u001b[39m(types)):\n\u001b[1;32m--> 136\u001b[0m   mapped \u001b[38;5;241m=\u001b[39m [\u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct]\n\u001b[0;32m    138\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:143\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[0;32m    141\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(mapped)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Singleton\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:489\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[1;34m(self, split, shuffle_files, batch_size, decoders, as_supervised, in_memory)\u001b[0m\n\u001b[0;32m    486\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;28mnext\u001b[39m(dataset_utils\u001b[38;5;241m.\u001b[39mas_numpy(dataset)))\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m      \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size:\n\u001b[0;32m    493\u001b[0m   \u001b[38;5;66;03m# Use padded_batch so that features with unknown shape are supported.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mpadded_batch(batch_size, dataset\u001b[38;5;241m.\u001b[39moutput_shapes)\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:832\u001b[0m, in \u001b[0;36mFileAdapterBuilder._as_dataset\u001b[1;34m(self, split, decoders, shuffle_files)\u001b[0m\n\u001b[0;32m    828\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfrecords_reader\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    829\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, split, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues(), shuffle_files)\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    831\u001b[0m   \u001b[38;5;66;03m# Resolve all the named split tree by real ones\u001b[39;00m\n\u001b[1;32m--> 832\u001b[0m   read_instruction \u001b[38;5;241m=\u001b[39m \u001b[43msplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_read_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m   \u001b[38;5;66;03m# Extract the list of SlicedSplitInfo objects containing the splits\u001b[39;00m\n\u001b[0;32m    834\u001b[0m   \u001b[38;5;66;03m# to use and their associated slice\u001b[39;00m\n\u001b[0;32m    835\u001b[0m   list_sliced_split_info \u001b[38;5;241m=\u001b[39m read_instruction\u001b[38;5;241m.\u001b[39mget_list_sliced_split_info()\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:359\u001b[0m, in \u001b[0;36mNamedSplit.get_read_instruction\u001b[1;34m(self, split_dict)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_read_instruction\u001b[39m(\u001b[38;5;28mself\u001b[39m, split_dict):\n\u001b[1;32m--> 359\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SplitReadInstruction(\u001b[43msplit_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\kelve\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:533\u001b[0m, in \u001b[0;36mSplitDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m    532\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(key) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid split \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Available splits are: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    534\u001b[0m         key, \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()))))\n\u001b[0;32m    535\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SplitDict, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mstr\u001b[39m(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Invalid split train[:10%]. Available splits are: ['train']\""
     ]
    }
   ],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)\n",
    "\n",
    "# extra code â€“ displays the first 9 images in the validation set\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "index = 0\n",
    "for image, label in valid_set_raw.take(9):\n",
    "    index += 1\n",
    "    plt.subplot(3, 3, index)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Class: {class_names[label]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # extra code â€“ resets layer name counter\n",
    "\n",
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "\n",
    "# extra code â€“ displays the first 9 images in the first batch of valid_set\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in valid_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow((X_batch[index] + 1) / 2)  # rescale to 0â€“1 for imshow()\n",
    "        plt.title(f\"Class: {class_names[y_batch[index]]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])\n",
    "\n",
    "# extra code â€“ displays the same first 9 images, after augmentation\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in valid_set.take(1):\n",
    "    X_batch_augmented = data_augmentation(X_batch, training=True)\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        # We must rescale the images to the 0-1 range for imshow(), and also\n",
    "        # clip the result to that range, because data augmentation may\n",
    "        # make some values go out of bounds (e.g., RandomContrast in this case).\n",
    "        plt.imshow(np.clip((X_batch_augmented[index] + 1) / 2, 0, 1))\n",
    "        plt.title(f\"Class: {class_names[y_batch[index]]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code â€“ ensures reproducibility\n",
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in zip(range(33), range(33, 66), range(66, 99), range(99, 132)):\n",
    "    for idx in indices:\n",
    "        print(f\"{idx:3}: {base_model.layers[idx].name:22}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[è¿”å›ž](readme.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
